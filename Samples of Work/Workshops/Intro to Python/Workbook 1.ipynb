{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workbook 1: Preprocessing Text Data\n",
    "### Summary:\n",
    "- Reading in text data \n",
    "    - txt\n",
    "    - pdf\n",
    "    - images\n",
    "    - word\n",
    "- Cleaning strings\n",
    "    - Clean est_price\n",
    "    - Regular expressions\n",
    "    - Clean desc_1\n",
    "- Tokenization\n",
    "- Removing stop words\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "- Creating a document-term matrix\n",
    "- Analyzing word counts and sentiment\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Workbook 0b we focused on wrangling quantitative data. In this workbook we'll transition to text data.\n",
    "\n",
    "The parallel of wrangling for qualitative data is **pre-processing**. This includes parsing, cleaning, tokenizing, removing stop words, stemming, and lemmatizing the data for analysis. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/bah17005/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import os \n",
    "\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in text data\n",
    "Workbook 0b covered loading data formatted as a .csv file. This is a common format for any data that lean quantitative. As we begin working working with more qualitative-leaning data, there are a variety of formats they may come in (e.g., .txt, .pdf, .docx). While the example in this workbook will continue using the coffee.csv file from Workbook 0b, in the first section, we will also load some other types of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to your file path:\n",
    "# option 1: if coffee.csv is saved in the same location as this workbook you can use path + file\n",
    "# option 2: remove parent + \"/Data/Coffee\" + file and paste a direct path encased in quotes (\" \" or ' ')\n",
    "# option 3: diy!\n",
    "path = os.getcwd() \n",
    "parent = os.path.abspath(os.path.join(path, os.pardir)) # this returns the parent folder of the cwd\n",
    "file_coffee = \"/coffee.csv\"\n",
    "full_path = parent + \"/Data/Coffee\" + file_coffee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slug</th>\n",
       "      <th>all_text</th>\n",
       "      <th>rating</th>\n",
       "      <th>roaster</th>\n",
       "      <th>name</th>\n",
       "      <th>region_africa_arabia</th>\n",
       "      <th>region_caribbean</th>\n",
       "      <th>region_central_america</th>\n",
       "      <th>region_hawaii</th>\n",
       "      <th>region_asia_pacific</th>\n",
       "      <th>...</th>\n",
       "      <th>agtron</th>\n",
       "      <th>aroma</th>\n",
       "      <th>acid</th>\n",
       "      <th>body</th>\n",
       "      <th>flavor</th>\n",
       "      <th>aftertaste</th>\n",
       "      <th>with_milk</th>\n",
       "      <th>desc_1</th>\n",
       "      <th>desc_2</th>\n",
       "      <th>desc_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.coffeereview.com/review/wilton-ben...</td>\n",
       "      <td>\\n\\n\\n95\\n\\n\\nJBC Coffee Roasters\\nWilton Ben...</td>\n",
       "      <td>95</td>\n",
       "      <td>JBC Coffee Roasters</td>\n",
       "      <td>Wilton Benitez Geisha</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>59/81</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Richly floral-toned, exceptionally sweet. Dist...</td>\n",
       "      <td>Produced by Wilton Benitez of Macarena Farm en...</td>\n",
       "      <td>A nuanced, complex experimentally processed Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.coffeereview.com/review/colombia-c...</td>\n",
       "      <td>\\n\\n\\n95\\n\\n\\nBird Rock Coffee Roasters\\nColo...</td>\n",
       "      <td>95</td>\n",
       "      <td>Bird Rock Coffee Roasters</td>\n",
       "      <td>Colombia Cerro Azul Geisha</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>62/80</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Richly aromatic, chocolaty, fruit-toned. Dark ...</td>\n",
       "      <td>Produced by Rigoberto Herrera of Granja La Esp...</td>\n",
       "      <td>A trifecta of fruit, chocolate and flowers, bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.coffeereview.com/review/yirgacheff...</td>\n",
       "      <td>\\n\\n\\n94\\n\\n\\nRegent Coffee\\nYirgacheffe Meng...</td>\n",
       "      <td>94</td>\n",
       "      <td>Regent Coffee</td>\n",
       "      <td>Yirgacheffe Mengesha Natural</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>60/77</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>High-toned, fruit-driven. Boysenberry, pear, c...</td>\n",
       "      <td>Produced at Mengesha Farm from selections of i...</td>\n",
       "      <td>A fruit medley in a cup — think boysenberry an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.coffeereview.com/review/colombia-t...</td>\n",
       "      <td>\\n\\n\\n93\\n\\n\\nRegent Coffee\\nColombia Tolima ...</td>\n",
       "      <td>93</td>\n",
       "      <td>Regent Coffee</td>\n",
       "      <td>Colombia Tolima Finca El Mirador Washed Anaerobic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>59/79</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Delicately fruit-toned. Guava, ginger blossom,...</td>\n",
       "      <td>Produced by Victor Gutiérrez of Finca Mirador ...</td>\n",
       "      <td>An appealing washed anaerobic cup: deep-toned,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.coffeereview.com/review/panama-gei...</td>\n",
       "      <td>\\n\\n\\n94\\n\\n\\nTheory Coffee Roasters\\nPanama ...</td>\n",
       "      <td>94</td>\n",
       "      <td>Theory Coffee Roasters</td>\n",
       "      <td>Panama Geisha Finca Debra Symbiosis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>62/80</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Richly fruit-forward, floral-toned. Lychee, te...</td>\n",
       "      <td>Produced by Jamison Savage of Finca Debra enti...</td>\n",
       "      <td>A floral- and fruit-driven anaerobic natural P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                slug  \\\n",
       "0  https://www.coffeereview.com/review/wilton-ben...   \n",
       "1  https://www.coffeereview.com/review/colombia-c...   \n",
       "2  https://www.coffeereview.com/review/yirgacheff...   \n",
       "3  https://www.coffeereview.com/review/colombia-t...   \n",
       "4  https://www.coffeereview.com/review/panama-gei...   \n",
       "\n",
       "                                            all_text  rating  \\\n",
       "0   \\n\\n\\n95\\n\\n\\nJBC Coffee Roasters\\nWilton Ben...      95   \n",
       "1   \\n\\n\\n95\\n\\n\\nBird Rock Coffee Roasters\\nColo...      95   \n",
       "2   \\n\\n\\n94\\n\\n\\nRegent Coffee\\nYirgacheffe Meng...      94   \n",
       "3   \\n\\n\\n93\\n\\n\\nRegent Coffee\\nColombia Tolima ...      93   \n",
       "4   \\n\\n\\n94\\n\\n\\nTheory Coffee Roasters\\nPanama ...      94   \n",
       "\n",
       "                     roaster  \\\n",
       "0        JBC Coffee Roasters   \n",
       "1  Bird Rock Coffee Roasters   \n",
       "2              Regent Coffee   \n",
       "3              Regent Coffee   \n",
       "4     Theory Coffee Roasters   \n",
       "\n",
       "                                                name  region_africa_arabia  \\\n",
       "0                              Wilton Benitez Geisha                     0   \n",
       "1                         Colombia Cerro Azul Geisha                     0   \n",
       "2                       Yirgacheffe Mengesha Natural                     1   \n",
       "3  Colombia Tolima Finca El Mirador Washed Anaerobic                     0   \n",
       "4                Panama Geisha Finca Debra Symbiosis                     0   \n",
       "\n",
       "   region_caribbean  region_central_america  region_hawaii  \\\n",
       "0                 0                       0              0   \n",
       "1                 0                       0              0   \n",
       "2                 0                       0              0   \n",
       "3                 0                       0              0   \n",
       "4                 0                       1              0   \n",
       "\n",
       "   region_asia_pacific  ...  agtron  aroma  acid  body  flavor  aftertaste  \\\n",
       "0                    0  ...   59/81    9.0   9.0   9.0     9.0         9.0   \n",
       "1                    0  ...   62/80    9.0   9.0   9.0     9.0         9.0   \n",
       "2                    0  ...   60/77    9.0   9.0   9.0     9.0         8.0   \n",
       "3                    0  ...   59/79    9.0   9.0   8.0     9.0         8.0   \n",
       "4                    0  ...   62/80    9.0   9.0   9.0     9.0         8.0   \n",
       "\n",
       "   with_milk                                             desc_1  \\\n",
       "0        NaN  Richly floral-toned, exceptionally sweet. Dist...   \n",
       "1        NaN  Richly aromatic, chocolaty, fruit-toned. Dark ...   \n",
       "2        NaN  High-toned, fruit-driven. Boysenberry, pear, c...   \n",
       "3        NaN  Delicately fruit-toned. Guava, ginger blossom,...   \n",
       "4        NaN  Richly fruit-forward, floral-toned. Lychee, te...   \n",
       "\n",
       "                                              desc_2  \\\n",
       "0  Produced by Wilton Benitez of Macarena Farm en...   \n",
       "1  Produced by Rigoberto Herrera of Granja La Esp...   \n",
       "2  Produced at Mengesha Farm from selections of i...   \n",
       "3  Produced by Victor Gutiérrez of Finca Mirador ...   \n",
       "4  Produced by Jamison Savage of Finca Debra enti...   \n",
       "\n",
       "                                              desc_3  \n",
       "0  A nuanced, complex experimentally processed Co...  \n",
       "1  A trifecta of fruit, chocolate and flowers, bo...  \n",
       "2  A fruit medley in a cup — think boysenberry an...  \n",
       "3  An appealing washed anaerobic cup: deep-toned,...  \n",
       "4  A floral- and fruit-driven anaerobic natural P...  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for importing .csv files\n",
    "coffee = pd.read_csv(full_path)\n",
    "coffee[0:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing plain text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'slug,all_text,rating,roaster,name,region_africa_arabia,region_caribbean,region_central_america,region_hawaii,region_asia_pacific,region_south_america,type_espresso,type_organic,type_fair_trade,type_decaffeinated,type_best_value,type_pod_capsule,type_blend,type_estate,type_peaberry,type_barrel_aged,type_aged,location,origin,roast,est_price,review_date,agtron,aroma,acid,body,flavor,aftertaste,with_milk,desc_1,desc_2,desc_3\\nhttps://www.coffeereview.com/review/wilton-benitez-geisha/,\" \\n\\n\\n95\\n\\n\\nJBC Coffee Roasters\\nWilton Benitez Geisha\\n\\n\\n \\n\\n\\n\\n\\n\\nRoaster Location:\\nMadison, Wisconsin\\n\\n\\nCoffee Origin:\\nPiendamó, Cauca Department, Colombia\\n\\n\\nRoast Level:\\nMedium-Light\\n\\n\\nAgtron:\\n59/81\\n\\n\\nEst. Price:\\n$25.00/8 ounces\\n\\n\\n\\n\\n\\n\\nReview Date:\\nNovember 2022\\n\\n\\nAroma:\\n9\\n\\n\\nAcidity/Structure: 9\\n\\n\\nBody:\\n9\\t\\t\\t\\t\\t\\t\\n\\nFlavor:\\n9\\n\\n\\nAftertaste:\\n9\\n\\n\\n\\n\\nBlind Assessment: Richly floral-toned, exceptionally sweet. Distinct narcissus, cocoa nib, myrrh, blackberry, lemon thyme in aroma and cup. Complex bittersweet structure with f'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for importing plain text (.txt) or binary files \n",
    "# open the file 1\n",
    "# read it in as a file 2\n",
    "# we can read in the .csv as a plain text file \n",
    "with open(full_path) as file: # 1\n",
    "   coffee2 = file.read() # 2\n",
    "\n",
    "# this is an equivelant code ^\n",
    "coffee2 = open(full_path).read()\n",
    "\n",
    "# ^ reads the data in as one large string so coffee[0:1000] gives the first  characters\n",
    "coffee2[0:1000]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Review\\nComplexity of coffee ﬂavor: A compositional and sensory perspective\\nWenny B. Sunarharuma,b, David J. Williamsc, Heather E. Smytha,⁎\\naQueensland Alliance for Agriculture and Food Innovation (QAAFI), The University of Queensland, PO Box 156 Archer ﬁeld BC, Queensland 4108, Australia\\nbDepartment of Food Science and Technology, Faculty of Agricultural Technology, University of Brawijaya, JL. Veteran Malang 65145, Indonesia\\ncAgri-Science Queensland, Department of Agriculture, Fisheries and Forestry (DAFF), PO Box 156, Archer ﬁeld BC, Queensland 4108, Australia\\nabstract article info\\nArticle history:\\nReceived 30 November 2013Accepted 23 February 2014Available online 1 March 2014\\nKeywords:\\nCoffee\\nFlavorCoffea arabicaAromaSensoryReviewFor the consumer, ﬂavor is arguably the most important aspect of a good coffee. Coffee ﬂavor is extremely\\ncomplex and arises from numerous chemical, biological and physical in ﬂuences of cultivar, coffee cherry maturity,\\ngeographical growing location, produ'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for importing pdf files\n",
    "# install the package in terminal:\n",
    "# pip3 install PyPDF2\n",
    "\n",
    "# import the package\n",
    "import PyPDF2 as pdf\n",
    "\n",
    "# new path - pdf article on coffee flavor profiles\n",
    "pdf_path = parent + \"/Data/Coffee\" + \"/coffee_flavor.pdf\"\n",
    "\n",
    "# open the file, \"rb\" = read, binary 1\n",
    "# call the pdf reader for pdf1 2\n",
    "# initiate a new string object to save the pdf textin 3\n",
    "# for each page of the pages the reader detected in the pdf 4\n",
    "# add to coffee_pdf the extracted text from the page + a new line 5\n",
    "pdf1 = open(pdf_path, \"rb\") # 1\n",
    "reader = pdf.PdfReader(pdf1) # 2\n",
    "\n",
    "coffee_pdf = \"\" # 3\n",
    "for page in reader.pages: # 4\n",
    "    coffee_pdf = coffee_pdf + page.extract_text() + \"\\n\" # 5\n",
    "\n",
    "# ^ reads the data in as one large string so coffee2[0:1000] gives the first 1000 characters\n",
    "coffee_pdf[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 15:24:02,610 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server-standard/2.6.0/tika-server-standard-2.6.0.jar to /var/folders/jb/__4n695j15169dhq5xqgt4l40000gp/T/tika-server.jar.\n",
      "2023-04-05 15:24:07,483 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server-standard/2.6.0/tika-server-standard-2.6.0.jar.md5 to /var/folders/jb/__4n695j15169dhq5xqgt4l40000gp/T/tika-server.jar.md5.\n",
      "2023-04-05 15:24:07,913 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nComplexity of coffee flavor: A compositional and sensory perspective\\n\\n\\nFood Research International 62 (2014) 315–325\\n\\nContents lists available at ScienceDirect\\n\\nFood Research International\\n\\nj ourna l homepage: www.e lsev ie r .com/ locate / foodres\\nReview\\nComplexity of coffee flavor: A compositional and sensory perspective\\nWenny B. Sunarharum a,b, David J. Williams c, Heather E. Smyth a,⁎\\na Queensland Alliance for Agriculture and Food Innovation (QAAFI), The University of Queensland, PO Box 156 Archerfield BC, Queensland 4108, Australia\\nb Department of Food Science and Technology, Faculty of Agricultural Technology, University of Brawijaya, JL. Veteran Malang 65145, Indonesia\\nc Agri-Science Queensland, Department of Agriculture, Fisheries and Forestry (DAFF), PO Box 156, Archerfield BC, Queensland 4108, Australia\\n⁎ Corresponding author. Tel.: +61 7 32766035.\\nE-mail address: h.smyth@uq.edu.au (H.E. Smyth).\\n\\nhttp://dx.doi.org/10.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for importing pdf files\n",
    "# install the package in terminal:\n",
    "# pip3 install tika\n",
    "\n",
    "# import the package\n",
    "import tika\n",
    "tika.initVM()\n",
    "from tika import parser\n",
    "\n",
    "# parse the pdf into metadata, content, and status\n",
    "parsed = parser.from_file(pdf_path)\n",
    "# view the metadata\n",
    "parsed[\"metadata\"]\n",
    "# view the status\n",
    "parsed[\"status\"]\n",
    "\n",
    "# save the content into a new object\n",
    "coffee_pdf2 = parsed[\"content\"]\n",
    "\n",
    "# ^ reads the data in as one large string so coffee2[0:1000] gives the first 1000 characters\n",
    "coffee_pdf2[0:1000]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the many \\n at the beginning of the PDF when we use tika. \\n is a character code for a new line which is a type of white space. To remove this use .strip() which will remove any leading or trailing white space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Complexity of coffee flavor: A compositional and sensory perspective\\n\\n\\nFood Research International 62 (2014) 315–325\\n\\nContents lists available at ScienceDirect\\n\\nFood Research International\\n\\nj ourna l homepage: www.e lsev ie r .com/ locate / foodres\\nReview\\nComplexity of coffee flavor: A compositional and sensory perspective\\nWenny B. Sunarharum a,b, David J. Williams c, Heather E. Smyth a,⁎\\na Queensland Alliance for Agriculture and Food Innovation (QAAFI), The University of Queensland, PO Box 156 Archerfield BC, Queensland 4108, Australia\\nb Department of Food Science and Technology, Faculty of Agricultural Technology, University of Brawijaya, JL. Veteran Malang 65145, Indonesia\\nc Agri-Science Queensland, Department of Agriculture, Fisheries and Forestry (DAFF), PO Box 156, Archerfield BC, Queensland 4108, Australia\\n⁎ Corresponding author. Tel.: +61 7 32766035.\\nE-mail address: h.smyth@uq.edu.au (H.E. Smyth).\\n\\nhttp://dx.doi.org/10.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coffee_pdf2[0:1000].strip()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tika package can also integrate with Tesseract Optimal Character Recognition (OCR) to extract content from images, older PDFs (which are rendered as images), or even webpages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Menu — Dose Coffee\\n\\n\\n\\n    \\n\\n\\n    \\n  \\n\\n    \\n      \\n        \\n          \\n            \\n          \\n        \\n      \\n    \\n\\n    \\n\\n    \\n\\n  \\n\\n  \\n    \\n      \\n        \\n          \\n          \\n        \\n        \\n          \\n          \\n        \\n        \\n          \\n          \\n        \\n        \\n          \\n          \\n        \\n        \\n          \\n          \\n        \\n        \\n          \\n          \\n        \\n        \\n          \\n          \\n        \\n      \\n    \\n\\n    \\n      \\n  \\n    \\n      Cart\\n\\n      \\n        \\n        \\n      \\n      \\n        \\n        \\n      \\n      \\n        \\n        \\n      \\n      \\n        \\n        \\n      \\n\\n      0\\n    \\n  \\n\\n    \\n\\n    \\n      \\n      \\n        \\n          \\n        \\n      \\n    \\n\\n  \\n\\n\\n  \\n    \\n      \\n        \\n          \\n  \\n    \\n      \\n        \\n          \\n            \\n              Home\\n            \\n          \\n        \\n      \\n    \\n    \\n  \\n    \\n      \\n        \\n          \\n            \\n              Online Ordering'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for importing webpages, images, or older PDFs \n",
    "# install tesseract ocr in terminal:\n",
    "# pip3 install tesseract tesseract-lang\n",
    "import requests\n",
    "\n",
    "link = \"http://dosenashville.com/menu\"\n",
    "\n",
    "# get the link\n",
    "response = requests.get(link) \n",
    "# instead of parser.from_file use parser.from_buffer\n",
    "parsed = parser.from_buffer(response.content) \n",
    "\n",
    "coffee_menu = parsed[\"content\"]\n",
    "\n",
    "coffee_menu[0:1000].strip()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Word documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coffee Flavor Terminology – A Coffee Taste Dictionary for the Noob\\nAugust 28, 2022\\xa0by\\xa0\\nCoffee Tasting Words\\nBeing able to name the flavors in coffee isn’t just another method for coffee professionals to display their knowledge. If you want to master coffee brewing, knowing how to taste coffee and having the proper vocabulary to express the flavors you distinguish, is an important instrument.\\nIt doesn’t matter if you find the coffee you just tasted appealing or not. Improving your ability to discern a coffee’s unique features, will help you discover more about about your coffee taste. As you progress, you will begin to observe what changes in your brewing method result in a better cup.\\nWe said it before, espresso is not the best brewing method to explore coffee flavors, since the heavy body masks many of the more delicate notes in coffee. However, you will still be able to detect hints of the origins, varietals, and processing method of your coffee beans. But let’s dive in and see what '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for importing word files\n",
    "# install docx in terminal:\n",
    "# pip3 install python-docx\n",
    "import docx\n",
    "\n",
    "# new path - word doc on coffee flavor terminology\n",
    "doc_path = parent + \"/Data/Coffee\" + \"/coffee_flavor.docx\"\n",
    "\n",
    "# call the document reader to the .docx file 1\n",
    "# initiate a new string object to save the doc text in 2\n",
    "# for each paragraph of the paragraphs the reader detected in the doc 3\n",
    "# add to coffee_doc the extracted text from the paragraph + a new line 4\n",
    "doc = docx.Document(doc_path) # 1\n",
    "\n",
    "coffee_doc = \"\" # 2\n",
    "for paragraph in doc.paragraphs: # 3\n",
    "    coffee_doc = coffee_doc + paragraph.text + \"\\n\" # 4\n",
    "\n",
    "# ^ reads the data in as one large string so coffee2[0:1000] gives the first 1000 characters\n",
    "coffee_doc[0:1000]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning strings\n",
    "### Cleaning est_price"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using data from .txt, .pdf, or .docx files, these data are loaded as a single string value and can require a bit of cleaning to produce a matrix or data frame that we can begin to process. While this workbook will focus on cleaning the est_price and desc_1 columns from from coffee.csv, the same principles apply will apply to other types of string. \n",
    "\n",
    "Expect some trial and error. Especially when working with an unfamiliar set of data, you do not know what patterns exist in the data. What I find most helpful is printing the data somewhere I can easily reference so I can look for patterns, code based that a pattern, and then check if it worked. (We learned how to do this in Workbook 0b!)\n",
    "\n",
    "In the PDF, webpage, and Word document we uploaded, there are character codes (e.g., \\xa0, \\n, \\nb) that you will see in the string that you don't see when viewing the document or website. You may encounter different types of encoding depending on where the document is coming from, and how the reader (e.g., tika or PyPDF2) is set to translate them. These encoders include ASCII, Unicode, UTF-8, UTF-16, and HTML. Each encoder has different codes for each character in a document including different types of whitespace, lists, etc. If a document has UTF-8 code but the reader is trying to interpret ASCII code then it will attempt to find the closest equivelent in ASCII and if it can't will return an error (this happened in HW 0 with the scores data). \n",
    "\n",
    "You'll see these codes from the document appear as part of the string. Note that tika and PyPDF2 return slightly different strings because of how the interpreters are set to work. There are packages that you can use to remove these codes or you may want to remove these manually since they can serve as markers in your data that you can use to isolate the text you are trying to extract."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now back to the coffee data frame... it includes a column for the estimated price of each coffee, if we print the first 10 cases we find these prices are set in different units. One thing we may want to do is compare the price across the same units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     $25.00/8 ounces\n",
       "1     $59.00/8 ounces\n",
       "2    $20.50/12 ounces\n",
       "3    $20.50/12 ounces\n",
       "4     $45.00/4 ounces\n",
       "5    $40.00/200 grams\n",
       "6    $43.00/200 grams\n",
       "7    $25.00/12 ounces\n",
       "8     $20.00/6 ounces\n",
       "9    $40.00/12 ounces\n",
       "Name: est_price, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coffee[\"est_price\"][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate just the column we are working with\n",
    "price = pd.DataFrame(coffee[\"est_price\"])\n",
    "price = price.rename(columns = {0:\"est_price\"})\n",
    "\n",
    "price[\"est_price\"] = price[\"est_price\"].astype(\"string\")\n",
    "#print(price[\"est_price\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at the column, we can note a few things:\n",
    "- Some cases have a an abbreviation before the $ indicating the type of currency (e.g., NT, HKD) or anoter indicator of currency (e.g., £)\n",
    "- Some cases have multiple prices listed, these are seperated by ;\n",
    "- Some cases list the price per bottle, these cases use a - as a seperator instead of whitespace (e.g., 12-ounce bottle). \n",
    "- Some cases have extra information encased in ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NT $1,500/250 grams\n",
      "$18.00/4-12-ounce bottles; 32 ounces/$12; 64-ounces/$20.00\n",
      "$15.00/20 ounces (2 types)\n",
      "£50.00/10 capsules\n",
      "€29.95/1 kilo (35.3 ounces)\n"
     ]
    }
   ],
   "source": [
    "# print the first case\n",
    "print(price[\"est_price\"][45])\n",
    "print(price[\"est_price\"][1400])\n",
    "print(price[\"est_price\"][1402])\n",
    "print(price[\"est_price\"][1427])\n",
    "print(price[\"est_price\"][1535])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When approaching cleaning text data, I like to start my tasks with the largest unit. For example, here we are interested in using a single price for comparison, but some cases have multiple prices, therefore this is the largest unit. This will keep us from performing unnecessary cleaning tasks before we narrow our cleaning procedures. So here's how I'm going to approach cleaning the things I noticed above:\n",
    "\n",
    "1. Isolate a single price for comparison\n",
    "2. Remove extra information in ()\n",
    "3. Isolate currency information before the numbers\n",
    "4. Isolate the numbers\n",
    "5. Isolate the unit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a few approaches here. One approach is to split the string at the ; since this seperates one price from another. When we do this, cases with multiple prices are split into a list. Each price therefore becomes a string in a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$18.00/4-12-ounce bottles', ' 32 ounces/$12', ' 64-ounces/$20.00']\n"
     ]
    }
   ],
   "source": [
    "# create a new variable in price with the split prices\n",
    "price[\"split_prices\"] = price[\"est_price\"].str.split(\";\")\n",
    "\n",
    "# this is a case with multiple prices that when split, turns into a list\n",
    "print(price[\"split_prices\"][1400])\n",
    "\n",
    "# initiate a new list 1\n",
    "# for each case in split_prices 2\n",
    "# if the case is missing, 3\n",
    "# append a blank value to the list 4\n",
    "# if the case is not missing, 5\n",
    "# append just the first string in the list 6\n",
    "# if there is just one price, this will keep it\n",
    "# if there's more than one price then it will only retain the first one\n",
    "\n",
    "est_price_split = [] # 1\n",
    "\n",
    "for each_price in price[\"split_prices\"]: # 2\n",
    "    if pd.isna(each_price) is True: # 3\n",
    "        est_price_split.append(None) # 4\n",
    "    elif pd.isna(each_price) is not True: # 5\n",
    "        est_price_split.append(each_price[0]) # 6\n",
    "\n",
    "# use this list to create a new column in our data frame\n",
    "price[\"first_price\"] = est_price_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove extra information in ()\n",
    "Now we have isolated a single price for comparison. The next step is to remove information contained in the () which is additional information that we do not need. Let's see what these cases look like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NA (available in store only)', '$3.00/sachet (plus one donated)', '$15.00/20 ounces (2 types)', '$45.95/8 ounces (currently on sale for $36.76)', '$13.99/12 ounces ($79.00/5 pounds)', '$13.99/12 ounces ($79.00/5 pounds)', '$21.00/12 ounces (includes shipping)', '€29.95/1 kilo (35.3 ounces)', '$39.95/8 ounces (packaged as a \"duo\" with Bourbon Rey Guatemala)', '$39.95/8 ounces (packaged as a \"duo\" with the Bourbon Rey Jamaica)', '$8.99/8 ounces (226 grams)', '$9.99/7 ounces (198 grams)', '$16.98/45 grams (approx. 9 servings)', '$12.00/25.4-ounce bottle (seasonal)', '$65.99/12 10.5-ounce bottles (shipping within California only)']\n"
     ]
    }
   ],
   "source": [
    "# create a new list to capture cases with parentheses 1\n",
    "# for each price in the variable where we isolated the first price 2\n",
    "# if the case is not missing 3\n",
    "# if there is an open parentheses found in each_price 4\n",
    "# find() will return the location in the substring and if the substring is not found it will return -1 (so any value greater than 0 means an open paren was detected)\n",
    "# append cases with open parentheses to the list 5\n",
    "\n",
    "in_paren = [] # 1\n",
    "\n",
    "for each_price in price[\"first_price\"]: # 2\n",
    "    if pd.isna(each_price) is not True: # 3\n",
    "        if each_price.find(\"(\") >= 0: # 4\n",
    "            in_paren.append(each_price) # 5\n",
    "\n",
    "print(in_paren)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that each case has different information contained within parentheses. We could use str.replace() to remove each individual case but this wouldn't be efficient or scalable. Instead we can use regular expressions to tackle this more efficiently."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular expressions\n",
    "**Regular expressions (regex)** are a sequence of characters or groupings that specifies a pattern in a string. \n",
    "\n",
    "Characters\n",
    "|regex|Character|\n",
    "|---|---|\n",
    "|'\\'|Escape character|\n",
    "|.|Any character except a new line|\n",
    "|^|Start of a string|\n",
    "|$|End of a string|\n",
    "|*|0 or more repetitions (ab* will match a, ab, abb, abbb, etc.; multiply b by anything >= 0)|\n",
    "|+|1 or more repetitions (ab+ will match ab, abb, abbb, etc.; multiply b by anything >= 1)|\n",
    "|?|0 or 1 repetitions (ab? will match a or ab; multiply b by 0 or 1)\n",
    "|{}|An exact number of copies (a{6} will match exactly 6 a's)|\n",
    "|{min, max}|A range of copies (a{3, 6} will match exactly 3-5 a's and a{3, } will match 3+ a's)|\n",
    "|\\d|Decimal digit|\n",
    "|\\D|Not a decimal digit|\n",
    "|\\w|Word character|\n",
    "|\\W|Not a word character|\n",
    "|\\s|Whitespace|\n",
    "|\\S|Not whitespace|\n",
    "|\\b|Word boundary|\n",
    "|\\B|Not a word boundary|\n",
    "\n",
    "Groupings\n",
    "|regex|Grouping|\n",
    "|---|---|\n",
    "|[]|Set of characters|\n",
    "|[^]|Characters not in brackets|\n",
    "||Either or|\n",
    "|()|Any regex in the parentheses|\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use regex search for any data with parentheses (similar to what we did to create in_paren). Instead of searching just for an open parenthesis, we can search for parentheses with some string patterns in them.\n",
    "\n",
    "Note that parentheses are a grouping regex. To look for parentheses we need to use \\( and \\) to tell the re package that we don't want to use () as regex but instead want to literally look for ( and )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]                                                    2262\n",
       "[$79.00/5 pounds]                                        2\n",
       "[available in store only]                                1\n",
       "[plus one donated]                                       1\n",
       "[2 types]                                                1\n",
       "[currently on sale for $36.76]                           1\n",
       "[includes shipping]                                      1\n",
       "[35.3 ounces]                                            1\n",
       "[packaged as a \"duo\" with Bourbon Rey Guatemala]         1\n",
       "[packaged as a \"duo\" with the Bourbon Rey Jamaica]       1\n",
       "[226 grams]                                              1\n",
       "[198 grams]                                              1\n",
       "[approx. 9 servings]                                     1\n",
       "[seasonal]                                               1\n",
       "[shipping within California only]                        1\n",
       "Name: first_price, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look for parentheses and not grouping regex use \\( and \\)\n",
    "# look for an open parenthesis \\(\n",
    "# after the open parenthesis, look for any regex in ()\n",
    "# any character except a new line .\n",
    "# with 0 or more repetitions of any character *\n",
    "# finally, look for the close parenthesis \\)\n",
    "price[\"first_price\"].str.findall(r\"\\((.*)\\)\").value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$15.00/20 ounces (2 types)\n"
     ]
    }
   ],
   "source": [
    "# print just one test case we know has parentheses\n",
    "print(price[\"first_price\"][1402])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jb/__4n695j15169dhq5xqgt4l40000gp/T/ipykernel_69468/1391224808.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  price[\"first_price\"] = price[\"first_price\"].str.replace(r\"\\((.*)\\)\", \"\")\n"
     ]
    }
   ],
   "source": [
    "# replace all the things we searched for above with a blank value \"\" \n",
    "price[\"first_price\"] = price[\"first_price\"].str.replace(r\"\\((.*)\\)\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$15.00/20 ounces \n"
     ]
    }
   ],
   "source": [
    "# print the test case to see that it worked\n",
    "print(price[\"first_price\"][1402])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isolate currency information before the numbers\n",
    "We can also use regular expressions to look for patterns related to currency.\n",
    "\n",
    "For example, when we search for the exact substring \"NT\" in our data, we find 547 cases. Where as when we use regular expressions to search just for occurances of NT appearing as the first part of the string, it only returns 544 cases. This means there are 3 cases where the substring NT is found in the string but not as the first part of the string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]      1730\n",
       "[NT]     547\n",
       "Name: first_price, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price[\"first_price\"].str.findall(\"NT\").value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]        1733\n",
       "[NT $]     528\n",
       "[NT$3]       3\n",
       "[NTD ]       3\n",
       "[NT$5]       2\n",
       "[NT$6]       2\n",
       "[NT$4]       2\n",
       "[NT$2]       1\n",
       "[NT$8]       1\n",
       "[NT$7]       1\n",
       "[NT 4]       1\n",
       "Name: first_price, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ^ = start of string\n",
    "# search for NT at the start of the string\n",
    "# followed by any 2 characters ..\n",
    "price[\"first_price\"].str.findall(r\"^NT..\").value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we start to pick up on some of the inconsistencies in how currency was input. This indicates that we'll need to do some cleaning before we can isolate it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[$]        1654\n",
       "[NT $]      528\n",
       "[]           34\n",
       "[CAD $]      20\n",
       "[NT$]        12\n",
       "[HKD $]       7\n",
       "[HK $]        5\n",
       "[AED $]       3\n",
       "[NTD $]       3\n",
       "[$NT$]        2\n",
       "[USD $]       2\n",
       "[IDR $]       2\n",
       "[KRW $]       2\n",
       "[US $]        1\n",
       "[AUD $]       1\n",
       "[THB $]       1\n",
       "Name: first_price, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look for some values at the start of the string ^\n",
    "# any character .\n",
    "# with 0 or more repetitions *\n",
    "# lastly look for a literal $ and don't use it as a regex i.e., \\$\n",
    "price[\"first_price\"].str.findall(r\"^.*\\$\").value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, note that most currencies end in a dollar sign, though not all. From the last chunk and this one, we see NT 4 which indicates the start of a price but no \\$ before it. We also see 34 cases where no $ was detected. These could be cases where the entire price is missing or typos just missing \\$ like NT 4.\n",
    "\n",
    "Next, note the typos and inconsistencies. \\$NT\\$ and NT\\$ each of which appear in our value counts as cases different from NT \\$. And \\$, US \\$, and USD $ all refer to US dollars. \n",
    "\n",
    "Finally, note the missing cases [ ]. If we were to split using \\$, this symbol would not be retained. Since the most of the USD cases do not have anything before the \\$, if we split on it these USD cases would appear the same as any missing cases (i.e., they would all be empty). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's correct the typos and inconsistencies in currency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jb/__4n695j15169dhq5xqgt4l40000gp/T/ipykernel_69468/3311093271.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^\\$NT\\$\", \"NT $\")\n",
      "/var/folders/jb/__4n695j15169dhq5xqgt4l40000gp/T/ipykernel_69468/3311093271.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^NT\\$\", \"NT $\")\n",
      "/var/folders/jb/__4n695j15169dhq5xqgt4l40000gp/T/ipykernel_69468/3311093271.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^NTD \\$\", \"NT $\")\n",
      "/var/folders/jb/__4n695j15169dhq5xqgt4l40000gp/T/ipykernel_69468/3311093271.py:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^US \\$\", \"USD $\")\n",
      "/var/folders/jb/__4n695j15169dhq5xqgt4l40000gp/T/ipykernel_69468/3311093271.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^\\$\", \"USD $\")\n",
      "/var/folders/jb/__4n695j15169dhq5xqgt4l40000gp/T/ipykernel_69468/3311093271.py:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^HK \\$\", \"HKD $\")\n",
      "/var/folders/jb/__4n695j15169dhq5xqgt4l40000gp/T/ipykernel_69468/3311093271.py:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^NA\", \"\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[USD $]    1657\n",
       "[NT $]      545\n",
       "[]           34\n",
       "[CAD $]      20\n",
       "[HKD $]      12\n",
       "[AED $]       3\n",
       "[IDR $]       2\n",
       "[KRW $]       2\n",
       "[AUD $]       1\n",
       "[THB $]       1\n",
       "Name: first_price, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace cases where the substring $NT$ or NT$ are found at the beginning of the string\n",
    "price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^\\$NT\\$\", \"NT $\")\n",
    "price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^NT\\$\", \"NT $\")\n",
    "price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^NTD \\$\", \"NT $\")\n",
    "\n",
    "# replace cases where the substring US $ or just $ are found at the beginning of the string\n",
    "price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^US \\$\", \"USD $\")\n",
    "price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^\\$\", \"USD $\")\n",
    "\n",
    "# replace HK with HKD\n",
    "price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^HK \\$\", \"HKD $\")\n",
    "\n",
    "# replace NA with blank\n",
    "price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^NA\", \"\")\n",
    "\n",
    "\n",
    "# check that it worked\n",
    "price[\"first_price\"].str.findall(r\"^.*\\$\").value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's dig into the 34 missing cases..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]         2244\n",
       "[¥1280]       4\n",
       "[£50/1]       3\n",
       "[RM 12]       2\n",
       "[¥ 2,4]       2\n",
       "[¥78/1]       2\n",
       "[£45/1]       2\n",
       "[#23.0]       1\n",
       "[18.00]       1\n",
       "[9.9 E]       1\n",
       "[£40.5]       1\n",
       "[See w]       1\n",
       "[€29.9]       1\n",
       "[£50.0]       1\n",
       "[¥ 1,5]       1\n",
       "[¥ 2,6]       1\n",
       "[RM 13]       1\n",
       "[500 p]       1\n",
       "[¥1,05]       1\n",
       "[¥2980]       1\n",
       "[£25.0]       1\n",
       "[¥88/1]       1\n",
       "[¥1680]       1\n",
       "[£100/]       1\n",
       "[NT 40]       1\n",
       "Name: first_price, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the longest currency abbreviation is 5 characters: 3 letters, a space, followed by $ \n",
    "# look for cases where there is no $ found in the first 5 [^]\n",
    "price[\"first_price\"].str.findall(r\"^[^\\$][^\\$][^\\$][^\\$][^\\$]\").value_counts()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see additional symbols for currency (£, ¥, €), some typos (#), and further inconsistencies. For example, one case lists the price as \"See website for more information.\" Because these cases are not consistent, I want to print them completely, to see if there is any additional information in the case that can help determine how they should be recoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             est_price                        split_prices  \\\n",
      "1685  See website for more information  [See website for more information]   \n",
      "\n",
      "                           first_price  \n",
      "1685  See website for more information  \n",
      "                est_price           split_prices          first_price\n",
      "1764  9.9 Euros/225 grams  [9.9 Euros/225 grams]  9.9 Euros/225 grams\n",
      "            est_price       split_prices      first_price\n",
      "1773  18.00/12 ounces  [18.00/12 ounces]  18.00/12 ounces\n",
      "             est_price        split_prices       first_price\n",
      "1945  #23.00/12 ounces  [#23.00/12 ounces]  #23.00/12 ounces\n",
      "               est_price           split_prices          first_price\n",
      "902  500 pesos/200 grams  [500 pesos/200 grams]  500 pesos/200 grams\n"
     ]
    }
   ],
   "source": [
    "# check the case that begins with See w\n",
    "print(price.loc[price[\"first_price\"].str.contains(r\"^See w\") == True])\n",
    "\n",
    "# probably $, but check the unit since might give an indication of the currency\n",
    "print(price.loc[price[\"first_price\"].str.contains(r\"^9.9\") == True])\n",
    "print(price.loc[price[\"first_price\"].str.contains(r\"^18\") == True])\n",
    "\n",
    "# check if this is supposed to be $ or £\n",
    "print(price.loc[price[\"first_price\"].str.contains(r\"^#\") == True])\n",
    "\n",
    "# check what currency this is supposed to be - maybe 500 pounds\n",
    "print(price.loc[price[\"first_price\"].str.contains(r\"^500 p\") == True])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recode the cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jb/__4n695j15169dhq5xqgt4l40000gp/T/ipykernel_69468/3751367626.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^NT [^\\$]\", \"NT $\")\n",
      "/var/folders/jb/__4n695j15169dhq5xqgt4l40000gp/T/ipykernel_69468/3751367626.py:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^RM [^\\$]\", \"NT $\")\n",
      "/var/folders/jb/__4n695j15169dhq5xqgt4l40000gp/T/ipykernel_69468/3751367626.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^18\", \"USD $18\")\n",
      "/var/folders/jb/__4n695j15169dhq5xqgt4l40000gp/T/ipykernel_69468/3751367626.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^#\", \"USD $\")\n",
      "/var/folders/jb/__4n695j15169dhq5xqgt4l40000gp/T/ipykernel_69468/3751367626.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^9.9 Euros\", \"€9.9\")\n",
      "/var/folders/jb/__4n695j15169dhq5xqgt4l40000gp/T/ipykernel_69468/3751367626.py:15: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^¥ \", \"¥\")\n",
      "/var/folders/jb/__4n695j15169dhq5xqgt4l40000gp/T/ipykernel_69468/3751367626.py:18: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^500 pesos\", \"MXN $500\")\n",
      "/var/folders/jb/__4n695j15169dhq5xqgt4l40000gp/T/ipykernel_69468/3751367626.py:19: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^¥\", \"JPY $\")\n",
      "/var/folders/jb/__4n695j15169dhq5xqgt4l40000gp/T/ipykernel_69468/3751367626.py:20: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^£\", \"GBP $\")\n",
      "/var/folders/jb/__4n695j15169dhq5xqgt4l40000gp/T/ipykernel_69468/3751367626.py:21: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^€\", \"EUR $\")\n"
     ]
    }
   ],
   "source": [
    "# change to NA if price isn't listed\n",
    "# in a real world scenario we could look these up and input them\n",
    "price[\"first_price\"] = price[\"first_price\"].str.replace(\"See website for more information\", \"\")\n",
    "\n",
    "# add $ to typos\n",
    "price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^NT [^\\$]\", \"NT $\")\n",
    "price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^RM [^\\$]\", \"NT $\")\n",
    "price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^18\", \"USD $18\")\n",
    "price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^#\", \"USD $\")\n",
    "\n",
    "# change to euro symbol\n",
    "price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^9.9 Euros\", \"€9.9\")\n",
    "\n",
    "# remove space after Yen\n",
    "price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^¥ \", \"¥\")\n",
    "\n",
    "# standardize how currency is reported\n",
    "price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^500 pesos\", \"MXN $500\")\n",
    "price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^¥\", \"JPY $\")\n",
    "price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^£\", \"GBP $\")\n",
    "price[\"first_price\"] = price[\"first_price\"].str.replace(r\"^€\", \"EUR $\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EUR $9.9/225 grams'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check a case to make sure our replacements worked\n",
    "price[\"first_price\"][1764]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the cases have a standardized method for reporting currency, we should not have any cases where there are no \\$ detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]    2277\n",
       "Name: first_price, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price[\"first_price\"].str.findall(r\"^[^\\$][^\\$][^\\$][^\\$][^\\$]\").value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all cases are standardized, we can use the \\$ as a value to split on to create a new variable for currency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new list to store the currency 1\n",
    "# create a new list to store the price and units 2\n",
    "# for each_price in first_price 3\n",
    "# if the case is missing 4\n",
    "# append a blank string 5 & 6\n",
    "# if it's not missing 7\n",
    "# split the price by \" $\" this creates a list of things before and after the split 8\n",
    "# append currency... this info is before the split so index [0] for the currency 9\n",
    "# append the remaining price and unit... this info is after the split 10\n",
    "\n",
    "currency = [] # 1\n",
    "price_unit = [] # 2\n",
    "\n",
    "for each_price in price[\"first_price\"]: # 3\n",
    "    if pd.isna(each_price) is True: # 4\n",
    "        currency.append(\"\") # 5\n",
    "        price_unit.append(\"\") # 6\n",
    "    elif pd.isna(each_price) is False: # 7\n",
    "        split_price = each_price.split(\" $\") # 8\n",
    "        currency.append(split_price[0]) # 9\n",
    "        price_unit.append(split_price[len(split_price)-1]) # 10\n",
    "\n",
    "# save the list as a new variable in our data frame\n",
    "price[\"currency\"] = currency"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split price_unit\n",
    "With the remaining information in price_unit it is formatted as price/unit. We can use the / to split in the same way we did the $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.00/8 ounces\n"
     ]
    }
   ],
   "source": [
    "# format of price_unit is price/unit\n",
    "print(price_unit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this loop follows the same format as the one above\n",
    "just_price = []\n",
    "unit = []\n",
    "\n",
    "for each_case in price_unit:\n",
    "    if pd.isna(each_case) is True: \n",
    "        currency.append(\"\") \n",
    "        price_unit.append(\"\") \n",
    "    elif pd.isna(each_case) is False: \n",
    "        split_price = each_case.split(\"/\") \n",
    "        just_price.append(split_price[0]) \n",
    "        unit.append(split_price[len(split_price)-1]) \n",
    "\n",
    "price[\"price\"] = just_price\n",
    "\n",
    "price[\"per_unit\"] = unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jb/__4n695j15169dhq5xqgt4l40000gp/T/ipykernel_69468/3112186452.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  price[\"price\"] = price[\"price\"].str.replace(r\"\\s\", \"\")\n",
      "/var/folders/jb/__4n695j15169dhq5xqgt4l40000gp/T/ipykernel_69468/3112186452.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  price[\"price\"] = price[\"price\"].str.replace(r\"[a-zA-Z]\", \"\")\n"
     ]
    }
   ],
   "source": [
    "# remove any commas, spaces, or letters\n",
    "price[\"price\"] = price[\"price\"].str.replace(\",\", \"\")\n",
    "price[\"price\"] = price[\"price\"].str.replace(r\"\\s\", \"\")\n",
    "price[\"price\"] = price[\"price\"].str.replace(r\"[a-zA-Z]\", \"\")\n",
    "\n",
    "# cast the price to a float\n",
    "price[\"price\"] = pd.to_numeric(price[\"price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.00      98\n",
       "20.00      88\n",
       "19.00      73\n",
       "25.00      59\n",
       "16.00      57\n",
       "           ..\n",
       "13.25       1\n",
       "3600.00     1\n",
       "65.95       1\n",
       "498.00      1\n",
       "18.40       1\n",
       "Name: price, Length: 351, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price[\"price\"].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isolate the unit\n",
    "The unit contains a number, space, and a string. Here we can use regex to grab the digits and words seperately. When looking at the unique per-unit values, there are some bottles and cans where the format is not uniform. When there are multiple bottles or cans, the quantity is listed first. So, here we use regex to list all the cases that don't begin with a digit. Some of these cases begin with whitespace, so first we'll get rid of these cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12 ounces                1092\n",
       "8 ounces                  369\n",
       "4 ounces                  127\n",
       "227 grams                  86\n",
       "16 ounces                  86\n",
       "                         ... \n",
       "50 ounces                   1\n",
       "Four 12-ounce bottles       1\n",
       "10.5-ounce bottle           1\n",
       "Four 8.4-ounce cans         1\n",
       "12 ounces online            1\n",
       "Name: per_unit, Length: 111, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price[\"per_unit\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "five 5-gram single-serve packets        4\n",
      "eight 3.3 gram packets                  2\n",
      "six 8-ounce cans                        1\n",
      "four 8-ounce cans                       1\n",
      "six 12-ounce cans                       1\n",
      "twelve 6-ounce cans                     1\n",
      "seven single-serve pouches              1\n",
      "can                                     1\n",
      "Four 12-ounce bottles                   1\n",
      "Four 8.4-ounce cans                     1\n",
      "sachet                                  1\n",
      "thirty 1.6-gram single-serve packets    1\n",
      "six 5-gram packets                      1\n",
      "six 5-gram single-serve packets         1\n",
      "eight 5-gram tubes                      1\n",
      "Name: per_unit, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jb/__4n695j15169dhq5xqgt4l40000gp/T/ipykernel_69468/1396590239.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  price[\"per_unit\"] = price[\"per_unit\"].str.replace(r\"^\\s\", \"\")\n"
     ]
    }
   ],
   "source": [
    "# for cases that begin with whitespace, remove that whitespace\n",
    "price[\"per_unit\"] = price[\"per_unit\"].str.replace(r\"^\\s\", \"\")\n",
    "\n",
    "# print all unique cases that don't begin with a digit\n",
    "print(price.loc[price[\"per_unit\"].str.contains(r\"^\\D\") == True][\"per_unit\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jb/__4n695j15169dhq5xqgt4l40000gp/T/ipykernel_69468/3052740715.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  price[\"per_unit\"] = price[\"per_unit\"].str.replace(\"eight 3.3 gram packets\", \"26.4 grams\")\n",
      "/var/folders/jb/__4n695j15169dhq5xqgt4l40000gp/T/ipykernel_69468/3052740715.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  price[\"per_unit\"] = price[\"per_unit\"].str.replace(\"Four 8.4-ounce cans\", \"33.6 ounces\")\n",
      "/var/folders/jb/__4n695j15169dhq5xqgt4l40000gp/T/ipykernel_69468/3052740715.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  price[\"per_unit\"] = price[\"per_unit\"].str.replace(\"thirty 1.6-gram single-serve packets\", \"48 grams\")\n",
      "/var/folders/jb/__4n695j15169dhq5xqgt4l40000gp/T/ipykernel_69468/3052740715.py:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  price[\"per_unit\"] = price[\"per_unit\"].str.replace(\"Four 8.4-ounce s\", \"33.6 ounces\")\n"
     ]
    }
   ],
   "source": [
    "price[\"per_unit\"] = price[\"per_unit\"].str.replace(\"five 5-gram single-serve packets\", \"25 grams\")\n",
    "price[\"per_unit\"] = price[\"per_unit\"].str.replace(\"eight 3.3 gram packets\", \"26.4 grams\")\n",
    "price[\"per_unit\"] = price[\"per_unit\"].str.replace(\"six 8-ounce cans\", \"48 ounces\")\n",
    "price[\"per_unit\"] = price[\"per_unit\"].str.replace(\"four 8-ounce cans\", \"32 ounces\")\n",
    "price[\"per_unit\"] = price[\"per_unit\"].str.replace(\"six 12-ounce cans\", \"72 ounces\")\n",
    "price[\"per_unit\"] = price[\"per_unit\"].str.replace(\"twelve 6-ounce cans\", \"72 ounces\")\n",
    "price[\"per_unit\"] = price[\"per_unit\"].str.replace(\"seven single-serve pouches\", \"\")\n",
    "price[\"per_unit\"] = price[\"per_unit\"].str.replace(\"can\", \"\")\n",
    "price[\"per_unit\"] = price[\"per_unit\"].str.replace(\"Four 12-ounce bottles\", \"48 ounces\")\n",
    "price[\"per_unit\"] = price[\"per_unit\"].str.replace(\"Four 8.4-ounce cans\", \"33.6 ounces\")\n",
    "price[\"per_unit\"] = price[\"per_unit\"].str.replace(\"sachet\", \"\")\n",
    "price[\"per_unit\"] = price[\"per_unit\"].str.replace(\"thirty 1.6-gram single-serve packets\", \"48 grams\")\n",
    "price[\"per_unit\"] = price[\"per_unit\"].str.replace(\"six 5-gram packets\", \"30 grams\")\n",
    "price[\"per_unit\"] = price[\"per_unit\"].str.replace(\"six 5-gram single-serve packets\", \"30 grams\")\n",
    "price[\"per_unit\"] = price[\"per_unit\"].str.replace(\"eight 5-gram tubes\", \"40 grams\")\n",
    "price[\"per_unit\"] = price[\"per_unit\"].str.replace(\"Four 8.4-ounce s\", \"33.6 ounces\")\n",
    "price[\"per_unit\"] = price[\"per_unit\"].str.replace(\"NA\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\n",
      "Name: per_unit, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check that the recodes worked\n",
    "# print all unique cases that don't begin with a digit\n",
    "print(price.loc[price[\"per_unit\"].str.contains(r\"^\\D\") == True][\"per_unit\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 0 or more repetitions of digits (\\d*) with 0 or 1 decimals or commas [.,]? in between\n",
    "price[\"per\"] = price[\"per_unit\"].str.extract(r\"(\\d*[.,]?\\d*)\")\n",
    "\n",
    "# cast to numeric\n",
    "price[\"per\"] = pd.to_numeric(price[\"per\"])\n",
    "\n",
    "# get anything that is lower or uppercase a-z for 1 or more repetitions\n",
    "price[\"unit\"] = price[\"per_unit\"].str.extract(r\"([a-zA-Z]+)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0     1110\n",
      "8.0       380\n",
      "4.0       128\n",
      "227.0      88\n",
      "16.0       86\n",
      "         ... \n",
      "275.0       1\n",
      "460.0       1\n",
      "70.0        1\n",
      "453.0       1\n",
      "25.4        1\n",
      "Name: per, Length: 64, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(price[\"per\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ounces      1816\n",
      "grams        405\n",
      "ounce         19\n",
      "capsules      10\n",
      "g              6\n",
      "ml             6\n",
      "gram           4\n",
      "pounds         3\n",
      "kilo           1\n",
      "sticks         1\n",
      "single         1\n",
      "Name: unit, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(price[\"unit\"].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not too much additional cleaning to do except making sure all the units that are on the same scale are reported in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jb/__4n695j15169dhq5xqgt4l40000gp/T/ipykernel_69468/3653063318.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  price[\"unit\"] = price[\"unit\"].str.replace(r\"gram$\", \"grams\")\n",
      "/var/folders/jb/__4n695j15169dhq5xqgt4l40000gp/T/ipykernel_69468/3653063318.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  price[\"unit\"] = price[\"unit\"].str.replace(r\"g$\", \"grams\")\n",
      "/var/folders/jb/__4n695j15169dhq5xqgt4l40000gp/T/ipykernel_69468/3653063318.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  price[\"unit\"] = price[\"unit\"].str.replace(r\"ounce$\", \"ounces\")\n"
     ]
    }
   ],
   "source": [
    "# recodes\n",
    "# grams\n",
    "# if we didn't use regex here it would replace the \"gram\" in \"grams\" with \"grams\" giving us \"gramss\"\n",
    "price[\"unit\"] = price[\"unit\"].str.replace(r\"gram$\", \"grams\")\n",
    "price[\"unit\"] = price[\"unit\"].str.replace(r\"g$\", \"grams\")\n",
    "\n",
    "# kilograms\n",
    "\n",
    "price[\"unit\"] = price[\"unit\"].str.replace(\"kilo\", \"kilograms\")\n",
    "\n",
    "# ounces\n",
    "price[\"unit\"] = price[\"unit\"].str.replace(r\"ounce$\", \"ounces\")\n",
    "\n",
    "# milliliters\n",
    "price[\"unit\"] = price[\"unit\"].str.replace(\"ml\", \"milliliters\")\n",
    "\n",
    "# unit unknown\n",
    "price[\"unit\"] = price[\"unit\"].str.replace(\"capsules\", \"\")\n",
    "price[\"unit\"] = price[\"unit\"].str.replace(\"sticks\", \"\")\n",
    "price[\"unit\"] = price[\"unit\"].str.replace(\"single\", \"\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our cases are now consistent! Though, they are not yet on the same scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ounces         1835\n",
       "grams           415\n",
       "                 12\n",
       "milliliters       6\n",
       "pounds            3\n",
       "kilograms         1\n",
       "Name: unit, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# value counts\n",
    "price[\"unit\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "USD    1659\n",
       "NT      549\n",
       "CAD      20\n",
       "JPY      14\n",
       "HKD      12\n",
       "GBP       9\n",
       "          6\n",
       "AED       3\n",
       "EUR       2\n",
       "IDR       2\n",
       "KRW       2\n",
       "          1\n",
       "MXN       1\n",
       "AUD       1\n",
       "THB       1\n",
       "Name: currency, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price[\"currency\"].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get them all on the same scale, we can create a new column for exchange rate which will help us convert all our prices to the same currency. We can do the same with the units to conver everything to the same unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initate the variable for exchange rate\n",
    "price[\"price_multiplier\"] = None\n",
    "\n",
    "# create a dictinary for converting to USD\n",
    "exchange_rate = {\"USD\" : 1, \n",
    "                 \"NT\" : 0.033,\n",
    "                 \"CAD\" : 0.73,\n",
    "                 \"JPY\" : 0.0075,\n",
    "                 \"GBP\" : 1.22,\n",
    "                 \"HKD\" : 0.13,\n",
    "                 \"AED\" : 0.27,\n",
    "                 \"KRW\" : 0.00077,\n",
    "                 \"EUR\" : 1.07,\n",
    "                 \"IDR\" : 0.000065,\n",
    "                 \"MXN\" : 0.053,\n",
    "                 \"AUD\" : 0.67,\n",
    "                 \"THB\" : 0.029}\n",
    "\n",
    "# recode the variable\n",
    "price = price.assign(price_multiplier = price[\"currency\"].map(exchange_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initate the unit multiplier\n",
    "price[\"unit_multiplier\"] = None\n",
    "\n",
    "# create a dictionary for converting to ounces\n",
    "unit_conversion = {\"ounces\" : 1,\n",
    "                   \"grams\" :.035274,\n",
    "                   \"pounds\" : 0.00220462,\n",
    "                   \"kilograms\" : 0.001,\n",
    "                   \"milliliters\" : 0.033814}\n",
    "\n",
    "# recode the variable\n",
    "price = price.assign(unit_multiplier = price[\"unit\"].map(unit_conversion))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create a new variable that represents the price in USD per ounce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>est_price</th>\n",
       "      <th>split_prices</th>\n",
       "      <th>first_price</th>\n",
       "      <th>currency</th>\n",
       "      <th>price</th>\n",
       "      <th>per_unit</th>\n",
       "      <th>per</th>\n",
       "      <th>unit</th>\n",
       "      <th>price_multiplier</th>\n",
       "      <th>unit_multiplier</th>\n",
       "      <th>usd_per_ounce</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>$24.50/12 ounces</td>\n",
       "      <td>[$24.50/12 ounces]</td>\n",
       "      <td>USD $24.50/12 ounces</td>\n",
       "      <td>USD</td>\n",
       "      <td>24.50</td>\n",
       "      <td>12 ounces</td>\n",
       "      <td>12.0</td>\n",
       "      <td>ounces</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>$23.00/12 ounces</td>\n",
       "      <td>[$23.00/12 ounces]</td>\n",
       "      <td>USD $23.00/12 ounces</td>\n",
       "      <td>USD</td>\n",
       "      <td>23.00</td>\n",
       "      <td>12 ounces</td>\n",
       "      <td>12.0</td>\n",
       "      <td>ounces</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>$18.99/8 ounces</td>\n",
       "      <td>[$18.99/8 ounces]</td>\n",
       "      <td>USD $18.99/8 ounces</td>\n",
       "      <td>USD</td>\n",
       "      <td>18.99</td>\n",
       "      <td>8 ounces</td>\n",
       "      <td>8.0</td>\n",
       "      <td>ounces</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.373750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>$22.50/12 ounces</td>\n",
       "      <td>[$22.50/12 ounces]</td>\n",
       "      <td>USD $22.50/12 ounces</td>\n",
       "      <td>USD</td>\n",
       "      <td>22.50</td>\n",
       "      <td>12 ounces</td>\n",
       "      <td>12.0</td>\n",
       "      <td>ounces</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>$20.95/12 ounces</td>\n",
       "      <td>[$20.95/12 ounces]</td>\n",
       "      <td>USD $20.95/12 ounces</td>\n",
       "      <td>USD</td>\n",
       "      <td>20.95</td>\n",
       "      <td>12 ounces</td>\n",
       "      <td>12.0</td>\n",
       "      <td>ounces</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.745833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>$16.95/12 ounces</td>\n",
       "      <td>[$16.95/12 ounces]</td>\n",
       "      <td>USD $16.95/12 ounces</td>\n",
       "      <td>USD</td>\n",
       "      <td>16.95</td>\n",
       "      <td>12 ounces</td>\n",
       "      <td>12.0</td>\n",
       "      <td>ounces</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.412500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>$35.00/200 grams</td>\n",
       "      <td>[$35.00/200 grams]</td>\n",
       "      <td>USD $35.00/200 grams</td>\n",
       "      <td>USD</td>\n",
       "      <td>35.00</td>\n",
       "      <td>200 grams</td>\n",
       "      <td>200.0</td>\n",
       "      <td>grams</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.035274</td>\n",
       "      <td>4.961161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>$17.95/12 ounces</td>\n",
       "      <td>[$17.95/12 ounces]</td>\n",
       "      <td>USD $17.95/12 ounces</td>\n",
       "      <td>USD</td>\n",
       "      <td>17.95</td>\n",
       "      <td>12 ounces</td>\n",
       "      <td>12.0</td>\n",
       "      <td>ounces</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.495833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>$19.25/12 ounces</td>\n",
       "      <td>[$19.25/12 ounces]</td>\n",
       "      <td>USD $19.25/12 ounces</td>\n",
       "      <td>USD</td>\n",
       "      <td>19.25</td>\n",
       "      <td>12 ounces</td>\n",
       "      <td>12.0</td>\n",
       "      <td>ounces</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.604167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NT $375/8 ounces</td>\n",
       "      <td>[NT $375/8 ounces]</td>\n",
       "      <td>NT $375/8 ounces</td>\n",
       "      <td>NT</td>\n",
       "      <td>375.00</td>\n",
       "      <td>8 ounces</td>\n",
       "      <td>8.0</td>\n",
       "      <td>ounces</td>\n",
       "      <td>0.033</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.546875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>$19.00/175 grams</td>\n",
       "      <td>[$19.00/175 grams]</td>\n",
       "      <td>USD $19.00/175 grams</td>\n",
       "      <td>USD</td>\n",
       "      <td>19.00</td>\n",
       "      <td>175 grams</td>\n",
       "      <td>175.0</td>\n",
       "      <td>grams</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.035274</td>\n",
       "      <td>3.077945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>NT $340/8 ounces</td>\n",
       "      <td>[NT $340/8 ounces]</td>\n",
       "      <td>NT $340/8 ounces</td>\n",
       "      <td>NT</td>\n",
       "      <td>340.00</td>\n",
       "      <td>8 ounces</td>\n",
       "      <td>8.0</td>\n",
       "      <td>ounces</td>\n",
       "      <td>0.033</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.402500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>$24.00/12 ounces</td>\n",
       "      <td>[$24.00/12 ounces]</td>\n",
       "      <td>USD $24.00/12 ounces</td>\n",
       "      <td>USD</td>\n",
       "      <td>24.00</td>\n",
       "      <td>12 ounces</td>\n",
       "      <td>12.0</td>\n",
       "      <td>ounces</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>NT $900/4 ounces</td>\n",
       "      <td>[NT $900/4 ounces]</td>\n",
       "      <td>NT $900/4 ounces</td>\n",
       "      <td>NT</td>\n",
       "      <td>900.00</td>\n",
       "      <td>4 ounces</td>\n",
       "      <td>4.0</td>\n",
       "      <td>ounces</td>\n",
       "      <td>0.033</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.425000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>NT $400/8 ounces</td>\n",
       "      <td>[NT $400/8 ounces]</td>\n",
       "      <td>NT $400/8 ounces</td>\n",
       "      <td>NT</td>\n",
       "      <td>400.00</td>\n",
       "      <td>8 ounces</td>\n",
       "      <td>8.0</td>\n",
       "      <td>ounces</td>\n",
       "      <td>0.033</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>NT $459/8 ounces</td>\n",
       "      <td>[NT $459/8 ounces]</td>\n",
       "      <td>NT $459/8 ounces</td>\n",
       "      <td>NT</td>\n",
       "      <td>459.00</td>\n",
       "      <td>8 ounces</td>\n",
       "      <td>8.0</td>\n",
       "      <td>ounces</td>\n",
       "      <td>0.033</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.893375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NT $350/8 ounces</td>\n",
       "      <td>[NT $350/8 ounces]</td>\n",
       "      <td>NT $350/8 ounces</td>\n",
       "      <td>NT</td>\n",
       "      <td>350.00</td>\n",
       "      <td>8 ounces</td>\n",
       "      <td>8.0</td>\n",
       "      <td>ounces</td>\n",
       "      <td>0.033</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.443750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>$19.00/12 ounces</td>\n",
       "      <td>[$19.00/12 ounces]</td>\n",
       "      <td>USD $19.00/12 ounces</td>\n",
       "      <td>USD</td>\n",
       "      <td>19.00</td>\n",
       "      <td>12 ounces</td>\n",
       "      <td>12.0</td>\n",
       "      <td>ounces</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>NT $399/8 ounces</td>\n",
       "      <td>[NT $399/8 ounces]</td>\n",
       "      <td>NT $399/8 ounces</td>\n",
       "      <td>NT</td>\n",
       "      <td>399.00</td>\n",
       "      <td>8 ounces</td>\n",
       "      <td>8.0</td>\n",
       "      <td>ounces</td>\n",
       "      <td>0.033</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.645875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>NT $249/8 ounces</td>\n",
       "      <td>[NT $249/8 ounces]</td>\n",
       "      <td>NT $249/8 ounces</td>\n",
       "      <td>NT</td>\n",
       "      <td>249.00</td>\n",
       "      <td>8 ounces</td>\n",
       "      <td>8.0</td>\n",
       "      <td>ounces</td>\n",
       "      <td>0.033</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.027125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           est_price        split_prices           first_price currency  \\\n",
       "20  $24.50/12 ounces  [$24.50/12 ounces]  USD $24.50/12 ounces      USD   \n",
       "21  $23.00/12 ounces  [$23.00/12 ounces]  USD $23.00/12 ounces      USD   \n",
       "22   $18.99/8 ounces   [$18.99/8 ounces]   USD $18.99/8 ounces      USD   \n",
       "23  $22.50/12 ounces  [$22.50/12 ounces]  USD $22.50/12 ounces      USD   \n",
       "24  $20.95/12 ounces  [$20.95/12 ounces]  USD $20.95/12 ounces      USD   \n",
       "25  $16.95/12 ounces  [$16.95/12 ounces]  USD $16.95/12 ounces      USD   \n",
       "26  $35.00/200 grams  [$35.00/200 grams]  USD $35.00/200 grams      USD   \n",
       "27  $17.95/12 ounces  [$17.95/12 ounces]  USD $17.95/12 ounces      USD   \n",
       "28  $19.25/12 ounces  [$19.25/12 ounces]  USD $19.25/12 ounces      USD   \n",
       "29  NT $375/8 ounces  [NT $375/8 ounces]      NT $375/8 ounces       NT   \n",
       "30  $19.00/175 grams  [$19.00/175 grams]  USD $19.00/175 grams      USD   \n",
       "31  NT $340/8 ounces  [NT $340/8 ounces]      NT $340/8 ounces       NT   \n",
       "32  $24.00/12 ounces  [$24.00/12 ounces]  USD $24.00/12 ounces      USD   \n",
       "33  NT $900/4 ounces  [NT $900/4 ounces]      NT $900/4 ounces       NT   \n",
       "34  NT $400/8 ounces  [NT $400/8 ounces]      NT $400/8 ounces       NT   \n",
       "35  NT $459/8 ounces  [NT $459/8 ounces]      NT $459/8 ounces       NT   \n",
       "36  NT $350/8 ounces  [NT $350/8 ounces]      NT $350/8 ounces       NT   \n",
       "37  $19.00/12 ounces  [$19.00/12 ounces]  USD $19.00/12 ounces      USD   \n",
       "38  NT $399/8 ounces  [NT $399/8 ounces]      NT $399/8 ounces       NT   \n",
       "39  NT $249/8 ounces  [NT $249/8 ounces]      NT $249/8 ounces       NT   \n",
       "\n",
       "     price   per_unit    per    unit  price_multiplier  unit_multiplier  \\\n",
       "20   24.50  12 ounces   12.0  ounces             1.000         1.000000   \n",
       "21   23.00  12 ounces   12.0  ounces             1.000         1.000000   \n",
       "22   18.99   8 ounces    8.0  ounces             1.000         1.000000   \n",
       "23   22.50  12 ounces   12.0  ounces             1.000         1.000000   \n",
       "24   20.95  12 ounces   12.0  ounces             1.000         1.000000   \n",
       "25   16.95  12 ounces   12.0  ounces             1.000         1.000000   \n",
       "26   35.00  200 grams  200.0   grams             1.000         0.035274   \n",
       "27   17.95  12 ounces   12.0  ounces             1.000         1.000000   \n",
       "28   19.25  12 ounces   12.0  ounces             1.000         1.000000   \n",
       "29  375.00   8 ounces    8.0  ounces             0.033         1.000000   \n",
       "30   19.00  175 grams  175.0   grams             1.000         0.035274   \n",
       "31  340.00   8 ounces    8.0  ounces             0.033         1.000000   \n",
       "32   24.00  12 ounces   12.0  ounces             1.000         1.000000   \n",
       "33  900.00   4 ounces    4.0  ounces             0.033         1.000000   \n",
       "34  400.00   8 ounces    8.0  ounces             0.033         1.000000   \n",
       "35  459.00   8 ounces    8.0  ounces             0.033         1.000000   \n",
       "36  350.00   8 ounces    8.0  ounces             0.033         1.000000   \n",
       "37   19.00  12 ounces   12.0  ounces             1.000         1.000000   \n",
       "38  399.00   8 ounces    8.0  ounces             0.033         1.000000   \n",
       "39  249.00   8 ounces    8.0  ounces             0.033         1.000000   \n",
       "\n",
       "    usd_per_ounce  \n",
       "20       2.041667  \n",
       "21       1.916667  \n",
       "22       2.373750  \n",
       "23       1.875000  \n",
       "24       1.745833  \n",
       "25       1.412500  \n",
       "26       4.961161  \n",
       "27       1.495833  \n",
       "28       1.604167  \n",
       "29       1.546875  \n",
       "30       3.077945  \n",
       "31       1.402500  \n",
       "32       2.000000  \n",
       "33       7.425000  \n",
       "34       1.650000  \n",
       "35       1.893375  \n",
       "36       1.443750  \n",
       "37       1.583333  \n",
       "38       1.645875  \n",
       "39       1.027125  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the new variable\n",
    "price[\"usd_per_ounce\"] = (price[\"price\"] * price[\"price_multiplier\"]) / (price[\"per\"] * price[\"unit_multiplier\"])\n",
    "\n",
    "price[20:40]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll return to the coffee data frame. Save the new usd_per_ounce column to coffee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee[\"usd_per_ounce\"] = price[\"usd_per_ounce\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning coffee descriptions (desc_1)\n",
    "There are 3 variables in coffee that contain descriptions of each coffee. We'll use the first description. Let's prepare these data by removing any punctuation and converting all words to lowercase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>desc_1</th>\n",
       "      <th>desc_2</th>\n",
       "      <th>desc_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Richly floral-toned, exceptionally sweet. Dist...</td>\n",
       "      <td>Produced by Wilton Benitez of Macarena Farm en...</td>\n",
       "      <td>A nuanced, complex experimentally processed Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Richly aromatic, chocolaty, fruit-toned. Dark ...</td>\n",
       "      <td>Produced by Rigoberto Herrera of Granja La Esp...</td>\n",
       "      <td>A trifecta of fruit, chocolate and flowers, bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>High-toned, fruit-driven. Boysenberry, pear, c...</td>\n",
       "      <td>Produced at Mengesha Farm from selections of i...</td>\n",
       "      <td>A fruit medley in a cup — think boysenberry an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Delicately fruit-toned. Guava, ginger blossom,...</td>\n",
       "      <td>Produced by Victor Gutiérrez of Finca Mirador ...</td>\n",
       "      <td>An appealing washed anaerobic cup: deep-toned,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Richly fruit-forward, floral-toned. Lychee, te...</td>\n",
       "      <td>Produced by Jamison Savage of Finca Debra enti...</td>\n",
       "      <td>A floral- and fruit-driven anaerobic natural P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>High-toned, richly bittersweet. Pomelo, raspbe...</td>\n",
       "      <td>Produced by Jamison Savage of Finca Debra enti...</td>\n",
       "      <td>A complex, multi-layered experimentally proces...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Crisply sweet-tart. Apricot, cocoa nib, agave ...</td>\n",
       "      <td>Produced by Jamison Savage of Finca Debra enti...</td>\n",
       "      <td>A balanced, richly sweet Panama Geisha cup, pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>High-toned, juicy-sweet. Mango, cocoa nib, mag...</td>\n",
       "      <td>Produced by smallholding farmers from trees of...</td>\n",
       "      <td>An invitingly elegant washed Ethiopia cup, def...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Richly spice-toned, floral-driven. Bergamot, l...</td>\n",
       "      <td>Produced by small-holding farmers largely from...</td>\n",
       "      <td>A lyrically composed Ethiopia anaerobic cup wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>High-toned, crisply sweet-tart. Lemongrass, co...</td>\n",
       "      <td>Produced by Gibran Leonardo Cervantes Covarrub...</td>\n",
       "      <td>A particularly fine Mexico Geisha: elegantly s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Crisply sweet-tart, richly fruit-toned Costa R...</td>\n",
       "      <td>Produced by Oscar and Francisca Chacon from tr...</td>\n",
       "      <td>This honey-processed Costa Rica shows a berry ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Very sweet, orangey and chocolate-toned. Ripe ...</td>\n",
       "      <td>Produced by Felipe Arcila of Jardines del Eden...</td>\n",
       "      <td>An experimentally processed fruit-macerated Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Deeply chocolaty, richly fruit-toned. Dark cho...</td>\n",
       "      <td>Produced by 22 women farmers of Finca Santa Ma...</td>\n",
       "      <td>A satisfying, richly chocolaty Colombia honey ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Crisply sweet, citrusy and bright. Orange zest...</td>\n",
       "      <td>Produced by Ismael Ramirez Roblero of Finca La...</td>\n",
       "      <td>A friendly, accessible, brightly sweet Mexico ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Rich-toned, deeply floral. White peach, tea ro...</td>\n",
       "      <td>Produced by the Reko Farming Cooperative from ...</td>\n",
       "      <td>A classic washed Yirgacheffe cup driven by not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Richly floral-toned, crisply sweet. Honeysuckl...</td>\n",
       "      <td>Produced by Tamiru Tadesse of Alo Coffee. Sout...</td>\n",
       "      <td>A floral-driven anaerobic Ethiopia cup free of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Elegantly sweet, floral-toned. Lilac, almond, ...</td>\n",
       "      <td>Produced by the METAD farm and mill in souther...</td>\n",
       "      <td>A brightly sweet, fragrantly floral-driven was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Delicately fruit-toned, tisane-like. Dried str...</td>\n",
       "      <td>Produced by Tamiru Tadesse of Alo Coffee. Sout...</td>\n",
       "      <td>A delicate, elegant, honey-processed Sidamo cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Delicate, richly aromatic. Pineapple, almond n...</td>\n",
       "      <td>Produced by Jorge Elias Rojas entirely of the ...</td>\n",
       "      <td>A lovely Colombia Geisha cup, finely tea-like ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Sweetly savory, deep-toned. Black currant, tom...</td>\n",
       "      <td>Produced by the Kariruki family entirely of th...</td>\n",
       "      <td>A savory-sweet Kenya cup with classic notes of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               desc_1  \\\n",
       "0   Richly floral-toned, exceptionally sweet. Dist...   \n",
       "1   Richly aromatic, chocolaty, fruit-toned. Dark ...   \n",
       "2   High-toned, fruit-driven. Boysenberry, pear, c...   \n",
       "3   Delicately fruit-toned. Guava, ginger blossom,...   \n",
       "4   Richly fruit-forward, floral-toned. Lychee, te...   \n",
       "5   High-toned, richly bittersweet. Pomelo, raspbe...   \n",
       "6   Crisply sweet-tart. Apricot, cocoa nib, agave ...   \n",
       "7   High-toned, juicy-sweet. Mango, cocoa nib, mag...   \n",
       "8   Richly spice-toned, floral-driven. Bergamot, l...   \n",
       "9   High-toned, crisply sweet-tart. Lemongrass, co...   \n",
       "10  Crisply sweet-tart, richly fruit-toned Costa R...   \n",
       "11  Very sweet, orangey and chocolate-toned. Ripe ...   \n",
       "12  Deeply chocolaty, richly fruit-toned. Dark cho...   \n",
       "13  Crisply sweet, citrusy and bright. Orange zest...   \n",
       "14  Rich-toned, deeply floral. White peach, tea ro...   \n",
       "15  Richly floral-toned, crisply sweet. Honeysuckl...   \n",
       "16  Elegantly sweet, floral-toned. Lilac, almond, ...   \n",
       "17  Delicately fruit-toned, tisane-like. Dried str...   \n",
       "18  Delicate, richly aromatic. Pineapple, almond n...   \n",
       "19  Sweetly savory, deep-toned. Black currant, tom...   \n",
       "\n",
       "                                               desc_2  \\\n",
       "0   Produced by Wilton Benitez of Macarena Farm en...   \n",
       "1   Produced by Rigoberto Herrera of Granja La Esp...   \n",
       "2   Produced at Mengesha Farm from selections of i...   \n",
       "3   Produced by Victor Gutiérrez of Finca Mirador ...   \n",
       "4   Produced by Jamison Savage of Finca Debra enti...   \n",
       "5   Produced by Jamison Savage of Finca Debra enti...   \n",
       "6   Produced by Jamison Savage of Finca Debra enti...   \n",
       "7   Produced by smallholding farmers from trees of...   \n",
       "8   Produced by small-holding farmers largely from...   \n",
       "9   Produced by Gibran Leonardo Cervantes Covarrub...   \n",
       "10  Produced by Oscar and Francisca Chacon from tr...   \n",
       "11  Produced by Felipe Arcila of Jardines del Eden...   \n",
       "12  Produced by 22 women farmers of Finca Santa Ma...   \n",
       "13  Produced by Ismael Ramirez Roblero of Finca La...   \n",
       "14  Produced by the Reko Farming Cooperative from ...   \n",
       "15  Produced by Tamiru Tadesse of Alo Coffee. Sout...   \n",
       "16  Produced by the METAD farm and mill in souther...   \n",
       "17  Produced by Tamiru Tadesse of Alo Coffee. Sout...   \n",
       "18  Produced by Jorge Elias Rojas entirely of the ...   \n",
       "19  Produced by the Kariruki family entirely of th...   \n",
       "\n",
       "                                               desc_3  \n",
       "0   A nuanced, complex experimentally processed Co...  \n",
       "1   A trifecta of fruit, chocolate and flowers, bo...  \n",
       "2   A fruit medley in a cup — think boysenberry an...  \n",
       "3   An appealing washed anaerobic cup: deep-toned,...  \n",
       "4   A floral- and fruit-driven anaerobic natural P...  \n",
       "5   A complex, multi-layered experimentally proces...  \n",
       "6   A balanced, richly sweet Panama Geisha cup, pr...  \n",
       "7   An invitingly elegant washed Ethiopia cup, def...  \n",
       "8   A lyrically composed Ethiopia anaerobic cup wi...  \n",
       "9   A particularly fine Mexico Geisha: elegantly s...  \n",
       "10  This honey-processed Costa Rica shows a berry ...  \n",
       "11  An experimentally processed fruit-macerated Co...  \n",
       "12  A satisfying, richly chocolaty Colombia honey ...  \n",
       "13  A friendly, accessible, brightly sweet Mexico ...  \n",
       "14  A classic washed Yirgacheffe cup driven by not...  \n",
       "15  A floral-driven anaerobic Ethiopia cup free of...  \n",
       "16  A brightly sweet, fragrantly floral-driven was...  \n",
       "17  A delicate, elegant, honey-processed Sidamo cu...  \n",
       "18  A lovely Colombia Geisha cup, finely tea-like ...  \n",
       "19  A savory-sweet Kenya cup with classic notes of...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coffee[[\"desc_1\", \"desc_2\", \"desc_3\"]][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make lower case\n",
    "def make_lower(text):\n",
    "    new_text = text.lower()\n",
    "    return new_text\n",
    "\n",
    "coffee[\"lower_desc_1\"] = coffee[\"desc_1\"].apply(make_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jb/__4n695j15169dhq5xqgt4l40000gp/T/ipykernel_69468/3536365620.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  coffee[\"clean_desc_1\"] = coffee[\"lower_desc_1\"].str.replace(r\"([^a-zA-Z\\s\\-'])\", \"\")\n"
     ]
    }
   ],
   "source": [
    "# replace anything that is not a lower or upper case letter, white space, apostrophe or hyphen with a blank\n",
    "coffee[\"clean_desc_1\"] = coffee[\"lower_desc_1\"].str.replace(r\"([^a-zA-Z\\s\\-'])\", \"\")\n",
    "\n",
    "# if we wanted to replace hyphens we could run the following\n",
    "#coffee[\"clean_desc_1\"] = coffee[\"clean_desc_1\"].str.replace(\"-\", \" \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "**Tokenization** is the process for breaking up text into smaller units. This example uses 1-word tokens though tokens can be made up of any unit smaller than the complete text, e.g., 2, 3, etc. words, phrases, or sentences. \n",
    "\n",
    "In this section, we'll clean and prep the coffee description text and then sperarate it into 1-word tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>desc_1</th>\n",
       "      <th>lower_desc_1</th>\n",
       "      <th>clean_desc_1</th>\n",
       "      <th>desc_1_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Richly floral-toned, exceptionally sweet. Dist...</td>\n",
       "      <td>richly floral-toned, exceptionally sweet. dist...</td>\n",
       "      <td>richly floral-toned exceptionally sweet distin...</td>\n",
       "      <td>[richly, floral-toned, exceptionally, sweet, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Richly aromatic, chocolaty, fruit-toned. Dark ...</td>\n",
       "      <td>richly aromatic, chocolaty, fruit-toned. dark ...</td>\n",
       "      <td>richly aromatic chocolaty fruit-toned dark cho...</td>\n",
       "      <td>[richly, aromatic, chocolaty, fruit-toned, dar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>High-toned, fruit-driven. Boysenberry, pear, c...</td>\n",
       "      <td>high-toned, fruit-driven. boysenberry, pear, c...</td>\n",
       "      <td>high-toned fruit-driven boysenberry pear cocoa...</td>\n",
       "      <td>[high-toned, fruit-driven, boysenberry, pear, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Delicately fruit-toned. Guava, ginger blossom,...</td>\n",
       "      <td>delicately fruit-toned. guava, ginger blossom,...</td>\n",
       "      <td>delicately fruit-toned guava ginger blossom co...</td>\n",
       "      <td>[delicately, fruit-toned, guava, ginger, bloss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Richly fruit-forward, floral-toned. Lychee, te...</td>\n",
       "      <td>richly fruit-forward, floral-toned. lychee, te...</td>\n",
       "      <td>richly fruit-forward floral-toned lychee tea r...</td>\n",
       "      <td>[richly, fruit-forward, floral-toned, lychee, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>High-toned, richly bittersweet. Pomelo, raspbe...</td>\n",
       "      <td>high-toned, richly bittersweet. pomelo, raspbe...</td>\n",
       "      <td>high-toned richly bittersweet pomelo raspberry...</td>\n",
       "      <td>[high-toned, richly, bittersweet, pomelo, rasp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Crisply sweet-tart. Apricot, cocoa nib, agave ...</td>\n",
       "      <td>crisply sweet-tart. apricot, cocoa nib, agave ...</td>\n",
       "      <td>crisply sweet-tart apricot cocoa nib agave syr...</td>\n",
       "      <td>[crisply, sweet-tart, apricot, cocoa, nib, aga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>High-toned, juicy-sweet. Mango, cocoa nib, mag...</td>\n",
       "      <td>high-toned, juicy-sweet. mango, cocoa nib, mag...</td>\n",
       "      <td>high-toned juicy-sweet mango cocoa nib magnoli...</td>\n",
       "      <td>[high-toned, juicy-sweet, mango, cocoa, nib, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Richly spice-toned, floral-driven. Bergamot, l...</td>\n",
       "      <td>richly spice-toned, floral-driven. bergamot, l...</td>\n",
       "      <td>richly spice-toned floral-driven bergamot lila...</td>\n",
       "      <td>[richly, spice-toned, floral-driven, bergamot,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>High-toned, crisply sweet-tart. Lemongrass, co...</td>\n",
       "      <td>high-toned, crisply sweet-tart. lemongrass, co...</td>\n",
       "      <td>high-toned crisply sweet-tart lemongrass cocoa...</td>\n",
       "      <td>[high-toned, crisply, sweet-tart, lemongrass, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              desc_1  \\\n",
       "0  Richly floral-toned, exceptionally sweet. Dist...   \n",
       "1  Richly aromatic, chocolaty, fruit-toned. Dark ...   \n",
       "2  High-toned, fruit-driven. Boysenberry, pear, c...   \n",
       "3  Delicately fruit-toned. Guava, ginger blossom,...   \n",
       "4  Richly fruit-forward, floral-toned. Lychee, te...   \n",
       "5  High-toned, richly bittersweet. Pomelo, raspbe...   \n",
       "6  Crisply sweet-tart. Apricot, cocoa nib, agave ...   \n",
       "7  High-toned, juicy-sweet. Mango, cocoa nib, mag...   \n",
       "8  Richly spice-toned, floral-driven. Bergamot, l...   \n",
       "9  High-toned, crisply sweet-tart. Lemongrass, co...   \n",
       "\n",
       "                                        lower_desc_1  \\\n",
       "0  richly floral-toned, exceptionally sweet. dist...   \n",
       "1  richly aromatic, chocolaty, fruit-toned. dark ...   \n",
       "2  high-toned, fruit-driven. boysenberry, pear, c...   \n",
       "3  delicately fruit-toned. guava, ginger blossom,...   \n",
       "4  richly fruit-forward, floral-toned. lychee, te...   \n",
       "5  high-toned, richly bittersweet. pomelo, raspbe...   \n",
       "6  crisply sweet-tart. apricot, cocoa nib, agave ...   \n",
       "7  high-toned, juicy-sweet. mango, cocoa nib, mag...   \n",
       "8  richly spice-toned, floral-driven. bergamot, l...   \n",
       "9  high-toned, crisply sweet-tart. lemongrass, co...   \n",
       "\n",
       "                                        clean_desc_1  \\\n",
       "0  richly floral-toned exceptionally sweet distin...   \n",
       "1  richly aromatic chocolaty fruit-toned dark cho...   \n",
       "2  high-toned fruit-driven boysenberry pear cocoa...   \n",
       "3  delicately fruit-toned guava ginger blossom co...   \n",
       "4  richly fruit-forward floral-toned lychee tea r...   \n",
       "5  high-toned richly bittersweet pomelo raspberry...   \n",
       "6  crisply sweet-tart apricot cocoa nib agave syr...   \n",
       "7  high-toned juicy-sweet mango cocoa nib magnoli...   \n",
       "8  richly spice-toned floral-driven bergamot lila...   \n",
       "9  high-toned crisply sweet-tart lemongrass cocoa...   \n",
       "\n",
       "                                       desc_1_tokens  \n",
       "0  [richly, floral-toned, exceptionally, sweet, d...  \n",
       "1  [richly, aromatic, chocolaty, fruit-toned, dar...  \n",
       "2  [high-toned, fruit-driven, boysenberry, pear, ...  \n",
       "3  [delicately, fruit-toned, guava, ginger, bloss...  \n",
       "4  [richly, fruit-forward, floral-toned, lychee, ...  \n",
       "5  [high-toned, richly, bittersweet, pomelo, rasp...  \n",
       "6  [crisply, sweet-tart, apricot, cocoa, nib, aga...  \n",
       "7  [high-toned, juicy-sweet, mango, cocoa, nib, m...  \n",
       "8  [richly, spice-toned, floral-driven, bergamot,...  \n",
       "9  [high-toned, crisply, sweet-tart, lemongrass, ...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a function to split the string into tokens\n",
    "def split_string(text):\n",
    "    tokens = text.split(\" \")\n",
    "    return tokens\n",
    "\n",
    "# apply this function to the cleaned description\n",
    "coffee[\"desc_1_tokens\"] = coffee[\"clean_desc_1\"].apply(split_string)\n",
    "\n",
    "# view the original desc_1 column, cleaned, and split side-by-side for the first 10 cases\n",
    "coffee[[\"desc_1\", \"lower_desc_1\", \"clean_desc_1\", \"desc_1_tokens\"]][0:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Toolkit (NLTK) also has a tokenizer which tokenizes based on white space and punctuation. Note that the NLTK tokenizer retains punctuation as a token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>desc_1</th>\n",
       "      <th>lower_desc_1</th>\n",
       "      <th>clean_desc_1</th>\n",
       "      <th>desc_1_tokens</th>\n",
       "      <th>desc_1_tokens_nltk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Richly floral-toned, exceptionally sweet. Dist...</td>\n",
       "      <td>richly floral-toned, exceptionally sweet. dist...</td>\n",
       "      <td>richly floral-toned exceptionally sweet distin...</td>\n",
       "      <td>[richly, floral-toned, exceptionally, sweet, d...</td>\n",
       "      <td>[richly, floral-toned, ,, exceptionally, sweet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Richly aromatic, chocolaty, fruit-toned. Dark ...</td>\n",
       "      <td>richly aromatic, chocolaty, fruit-toned. dark ...</td>\n",
       "      <td>richly aromatic chocolaty fruit-toned dark cho...</td>\n",
       "      <td>[richly, aromatic, chocolaty, fruit-toned, dar...</td>\n",
       "      <td>[richly, aromatic, ,, chocolaty, ,, fruit-tone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>High-toned, fruit-driven. Boysenberry, pear, c...</td>\n",
       "      <td>high-toned, fruit-driven. boysenberry, pear, c...</td>\n",
       "      <td>high-toned fruit-driven boysenberry pear cocoa...</td>\n",
       "      <td>[high-toned, fruit-driven, boysenberry, pear, ...</td>\n",
       "      <td>[high-toned, ,, fruit-driven, ., boysenberry, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Delicately fruit-toned. Guava, ginger blossom,...</td>\n",
       "      <td>delicately fruit-toned. guava, ginger blossom,...</td>\n",
       "      <td>delicately fruit-toned guava ginger blossom co...</td>\n",
       "      <td>[delicately, fruit-toned, guava, ginger, bloss...</td>\n",
       "      <td>[delicately, fruit-toned, ., guava, ,, ginger,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Richly fruit-forward, floral-toned. Lychee, te...</td>\n",
       "      <td>richly fruit-forward, floral-toned. lychee, te...</td>\n",
       "      <td>richly fruit-forward floral-toned lychee tea r...</td>\n",
       "      <td>[richly, fruit-forward, floral-toned, lychee, ...</td>\n",
       "      <td>[richly, fruit-forward, ,, floral-toned, ., ly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>High-toned, richly bittersweet. Pomelo, raspbe...</td>\n",
       "      <td>high-toned, richly bittersweet. pomelo, raspbe...</td>\n",
       "      <td>high-toned richly bittersweet pomelo raspberry...</td>\n",
       "      <td>[high-toned, richly, bittersweet, pomelo, rasp...</td>\n",
       "      <td>[high-toned, ,, richly, bittersweet, ., pomelo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Crisply sweet-tart. Apricot, cocoa nib, agave ...</td>\n",
       "      <td>crisply sweet-tart. apricot, cocoa nib, agave ...</td>\n",
       "      <td>crisply sweet-tart apricot cocoa nib agave syr...</td>\n",
       "      <td>[crisply, sweet-tart, apricot, cocoa, nib, aga...</td>\n",
       "      <td>[crisply, sweet-tart, ., apricot, ,, cocoa, ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>High-toned, juicy-sweet. Mango, cocoa nib, mag...</td>\n",
       "      <td>high-toned, juicy-sweet. mango, cocoa nib, mag...</td>\n",
       "      <td>high-toned juicy-sweet mango cocoa nib magnoli...</td>\n",
       "      <td>[high-toned, juicy-sweet, mango, cocoa, nib, m...</td>\n",
       "      <td>[high-toned, ,, juicy-sweet, ., mango, ,, coco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Richly spice-toned, floral-driven. Bergamot, l...</td>\n",
       "      <td>richly spice-toned, floral-driven. bergamot, l...</td>\n",
       "      <td>richly spice-toned floral-driven bergamot lila...</td>\n",
       "      <td>[richly, spice-toned, floral-driven, bergamot,...</td>\n",
       "      <td>[richly, spice-toned, ,, floral-driven, ., ber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>High-toned, crisply sweet-tart. Lemongrass, co...</td>\n",
       "      <td>high-toned, crisply sweet-tart. lemongrass, co...</td>\n",
       "      <td>high-toned crisply sweet-tart lemongrass cocoa...</td>\n",
       "      <td>[high-toned, crisply, sweet-tart, lemongrass, ...</td>\n",
       "      <td>[high-toned, ,, crisply, sweet-tart, ., lemong...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              desc_1  \\\n",
       "0  Richly floral-toned, exceptionally sweet. Dist...   \n",
       "1  Richly aromatic, chocolaty, fruit-toned. Dark ...   \n",
       "2  High-toned, fruit-driven. Boysenberry, pear, c...   \n",
       "3  Delicately fruit-toned. Guava, ginger blossom,...   \n",
       "4  Richly fruit-forward, floral-toned. Lychee, te...   \n",
       "5  High-toned, richly bittersweet. Pomelo, raspbe...   \n",
       "6  Crisply sweet-tart. Apricot, cocoa nib, agave ...   \n",
       "7  High-toned, juicy-sweet. Mango, cocoa nib, mag...   \n",
       "8  Richly spice-toned, floral-driven. Bergamot, l...   \n",
       "9  High-toned, crisply sweet-tart. Lemongrass, co...   \n",
       "\n",
       "                                        lower_desc_1  \\\n",
       "0  richly floral-toned, exceptionally sweet. dist...   \n",
       "1  richly aromatic, chocolaty, fruit-toned. dark ...   \n",
       "2  high-toned, fruit-driven. boysenberry, pear, c...   \n",
       "3  delicately fruit-toned. guava, ginger blossom,...   \n",
       "4  richly fruit-forward, floral-toned. lychee, te...   \n",
       "5  high-toned, richly bittersweet. pomelo, raspbe...   \n",
       "6  crisply sweet-tart. apricot, cocoa nib, agave ...   \n",
       "7  high-toned, juicy-sweet. mango, cocoa nib, mag...   \n",
       "8  richly spice-toned, floral-driven. bergamot, l...   \n",
       "9  high-toned, crisply sweet-tart. lemongrass, co...   \n",
       "\n",
       "                                        clean_desc_1  \\\n",
       "0  richly floral-toned exceptionally sweet distin...   \n",
       "1  richly aromatic chocolaty fruit-toned dark cho...   \n",
       "2  high-toned fruit-driven boysenberry pear cocoa...   \n",
       "3  delicately fruit-toned guava ginger blossom co...   \n",
       "4  richly fruit-forward floral-toned lychee tea r...   \n",
       "5  high-toned richly bittersweet pomelo raspberry...   \n",
       "6  crisply sweet-tart apricot cocoa nib agave syr...   \n",
       "7  high-toned juicy-sweet mango cocoa nib magnoli...   \n",
       "8  richly spice-toned floral-driven bergamot lila...   \n",
       "9  high-toned crisply sweet-tart lemongrass cocoa...   \n",
       "\n",
       "                                       desc_1_tokens  \\\n",
       "0  [richly, floral-toned, exceptionally, sweet, d...   \n",
       "1  [richly, aromatic, chocolaty, fruit-toned, dar...   \n",
       "2  [high-toned, fruit-driven, boysenberry, pear, ...   \n",
       "3  [delicately, fruit-toned, guava, ginger, bloss...   \n",
       "4  [richly, fruit-forward, floral-toned, lychee, ...   \n",
       "5  [high-toned, richly, bittersweet, pomelo, rasp...   \n",
       "6  [crisply, sweet-tart, apricot, cocoa, nib, aga...   \n",
       "7  [high-toned, juicy-sweet, mango, cocoa, nib, m...   \n",
       "8  [richly, spice-toned, floral-driven, bergamot,...   \n",
       "9  [high-toned, crisply, sweet-tart, lemongrass, ...   \n",
       "\n",
       "                                  desc_1_tokens_nltk  \n",
       "0  [richly, floral-toned, ,, exceptionally, sweet...  \n",
       "1  [richly, aromatic, ,, chocolaty, ,, fruit-tone...  \n",
       "2  [high-toned, ,, fruit-driven, ., boysenberry, ...  \n",
       "3  [delicately, fruit-toned, ., guava, ,, ginger,...  \n",
       "4  [richly, fruit-forward, ,, floral-toned, ., ly...  \n",
       "5  [high-toned, ,, richly, bittersweet, ., pomelo...  \n",
       "6  [crisply, sweet-tart, ., apricot, ,, cocoa, ni...  \n",
       "7  [high-toned, ,, juicy-sweet, ., mango, ,, coco...  \n",
       "8  [richly, spice-toned, ,, floral-driven, ., ber...  \n",
       "9  [high-toned, ,, crisply, sweet-tart, ., lemong...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a function to tokenize using nltk's tokenizer\n",
    "def tokenize(text):\n",
    "    new_text = nltk.tokenize.word_tokenize(text)\n",
    "    return new_text\n",
    "\n",
    "coffee[\"desc_1_tokens_nltk\"] = coffee[\"lower_desc_1\"].apply(tokenize)\n",
    "\n",
    "# view the original desc_1 column, cleaned, and split side-by-side for the first 10 cases\n",
    "coffee[[\"desc_1\", \"lower_desc_1\", \"clean_desc_1\", \"desc_1_tokens\", \"desc_1_tokens_nltk\"]][0:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "**Stop words** are common words or phrases that do not contribute much information to the analysis. Thus reducing the size of data set and training time (since we have fewer tokens to train). In this section we'll create our own list of stop words and NLTK's and work through how to remove them from our data.\n",
    "\n",
    "The size of the list of stop words will depend on the analysis and research questions. For example, perhaps we have a book review where Reader 1 says, \"The book was so good\" while Reader 2 says, \"The book was not good at all.\" After removing NLTK's stop words, both reviews would be reduced to \"book\" and \"good.\" This might be okay if the purpose of our analysis.\n",
    "\n",
    "For example, if the purpose of the analysis was analysing the themes that students talked about when reflecting on a course. From this we could determine that students gave feedback about the course resources. However, if we remove the stop words, we can't determine their sentiment towards these resources.\n",
    "\n",
    "Another example of why stop words might not be helpful is if we were analysing data from a publishing company where the reviewers were agents and editors who had read a submission. The company wants to use the data as an initial indicator for whether the company should invest in the book. For the purpose of this analysis, removing the complete list of NLTK's stop words would not be useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['book', 'good']\n",
      "['book', 'good']\n"
     ]
    }
   ],
   "source": [
    "review1 = [\"the\", \"book\", \"was\", \"not\", \"good\", \"at\", \"all\"]\n",
    "review2 = [\"the\", \"book\", \"was\", \"so\", \"good\"]\n",
    "\n",
    "# create a set of nltk's stop words\n",
    "nltk_stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "# create a function to remove stop words that requires 1 input, a list of tokens 1 \n",
    "# create a new list to save the output to 2\n",
    "# for each token in the list 3\n",
    "# if the token isn't in the list of stopwords 4\n",
    "# append the token to clean_tokens list 5 \n",
    "# return the list of clean tokens\n",
    "\n",
    "def remove_stop_words(token_list):  # 1\n",
    "    clean_tokens = [] # 2\n",
    "    for token in token_list: # 3\n",
    "        if token not in nltk_stop_words: # 4 \n",
    "            clean_tokens.append(token) # 5\n",
    "    return clean_tokens\n",
    "\n",
    "print(remove_stop_words(review1))\n",
    "print(remove_stop_words(review2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK's list of stop words includes 179 frequently used words which include articles, prepositions, pronouns, conjunctions, etc. Another package with stop words is spaCy which has an even longer list of stop words (326). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "{'my', 'your', 'as', 'further', 'now', 'so', 'during', 'ours', 'yours', 'while', 'here', 'hers', 'didn', 'at', 'y', \"didn't\", \"couldn't\", 'own', 'herself', 'aren', 'i', 'be', \"don't\", 'myself', 'have', 'm', 'any', 'd', 'her', 'by', \"weren't\", 'how', \"you'd\", 'again', 'or', 'off', 'theirs', 'with', 'too', \"you've\", 'yourselves', 'needn', 'same', 'shouldn', 'are', 'of', 'you', 'o', 'hasn', 'over', 'up', 'each', 'been', 'a', 'and', 'our', 'most', 'if', 'nor', 've', \"haven't\", 'whom', 'did', \"won't\", 'more', 'than', 'such', 'had', 'were', 's', 'until', 'before', \"mustn't\", 'mustn', 'their', 'that', 'ourselves', \"should've\", 'don', \"aren't\", 'them', 'very', 'its', 'against', 'who', 'does', \"you'll\", 'into', 'being', 'below', 'in', \"it's\", \"needn't\", 're', \"you're\", 'ain', 'to', 'mightn', \"shan't\", 'is', \"doesn't\", 'we', 'his', 'then', 'isn', 'was', 'once', 'these', 'on', \"wouldn't\", 'himself', 'the', 'am', \"she's\", 'll', \"mightn't\", 'can', 'itself', 'shan', 'just', 'should', 'doesn', 'this', 'doing', 'wouldn', \"wasn't\", 'after', 'some', 'all', 'why', 'other', 'no', \"hadn't\", 'where', 't', 'because', \"isn't\", 'yourself', 'do', 'it', 'weren', 'what', 'when', \"hasn't\", 'through', 'ma', 'both', 'wasn', 'for', 'out', \"shouldn't\", 'above', 'she', 'there', 'not', 'only', 'having', 'an', 'which', 'down', 'themselves', 'between', 'will', 'from', 'has', 'under', 'haven', 'they', 'him', \"that'll\", 'couldn', 'won', 'few', 'those', 'but', 'me', 'he', 'about', 'hadn'}\n"
     ]
    }
   ],
   "source": [
    "print(len(nltk_stop_words))\n",
    "print(nltk_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326\n",
      "{'as', 'further', 'now', 'so', 'during', 'while', 'former', 'be', '’ll', 'her', 'well', 'almost', '‘s', 'again', 'whole', \"'m\", 'twenty', 'without', 'however', 'whereby', 'empty', 'twelve', 'been', 'elsewhere', 'many', 'than', 'except', 'such', 'noone', 'onto', 'whereas', 'their', 'even', 'back', 'who', 'thus', 'anywhere', 'last', 'serious', \"'s\", 'his', 'we', 'somewhere', 'on', 'whereupon', 'am', 'either', 'hundred', 'already', 'just', 'behind', 'may', 'doing', 'name', 'after', 'some', 'amongst', 'do', 'see', 'what', 'when', 'anyone', 'several', 'anything', 'has', 'they', 'whether', \"'ve\", 'call', 'else', 'sixty', 'whose', 'my', 'your', '’s', 'here', 'yet', 'herself', 'anyhow', 'somehow', 'how', 'or', \"'d\", 'toward', 'same', 'are', 'of', 'up', '‘m', 'most', 'moreover', 'nor', 'thereby', 'otherwise', 'much', 'via', 'nothing', 'perhaps', 'were', 'rather', 'before', 'since', 'whenever', 'very', 'against', 'does', 'namely', 'being', 'beside', 'something', 'seemed', 'to', 'first', 'whatever', '‘d', 'whereafter', 'meanwhile', 'amount', 'please', 'say', 'should', 'no', 'because', 'yourself', 'side', 'still', 'eleven', 'latterly', 'an', 'everywhere', 'which', 'between', 'also', 'under', 'though', 'part', 'him', '’ve', 'those', 'hence', 'me', \"'ll\", 'anyway', 'must', 'about', 'herein', 'towards', 'become', 'ours', 'least', 'hers', 'own', 'often', 'could', 'using', 'enough', 'sometimes', '‘re', 'four', 'together', 'two', 'with', 'ca', 'over', 'nevertheless', 'and', '’re', 'our', 'if', 'more', 'had', 'put', 'within', 'that', 'wherein', 'them', 'never', 'becoming', 'into', 'front', 'go', 'hereafter', 'therein', 'in', 'someone', 'quite', 're', 'is', 'eight', 'then', 'once', 'these', 'himself', 'the', 'afterwards', 'thereafter', 'fifty', 'mine', 'latter', 'can', 'itself', 'three', 'alone', '’m', 'this', 'cannot', 'other', 'it', 'get', 'another', 'forty', 'per', 'mostly', 'third', 'for', 'above', 'there', 'throughout', 'only', 'will', 'from', 'made', '‘ll', 'move', 'few', 'every', 'but', 'became', 'he', 'none', 'thru', 'thence', 'bottom', 'others', 'across', 'yours', 'might', 'at', 'nobody', 'i', 'myself', 'have', 'seems', 'n’t', 'any', 'by', 'give', 'whoever', 'off', 'done', 'too', 'hereupon', 'ten', 'yourselves', '‘ve', 'one', 'although', 'you', 'among', 'each', 'a', 'formerly', 'whom', 'did', 'nowhere', 'show', 'upon', 'until', 'indeed', 'ourselves', 'unless', 'regarding', 'its', 'n‘t', 'take', 'always', 'therefore', 'below', 'seem', 'keep', 'whence', 'was', 'whither', 'hereby', 'everything', 'fifteen', 'everyone', 'besides', 'various', 'really', 'beforehand', \"'re\", 'five', 'top', '’d', 'all', 'less', 'why', 'where', 'sometime', \"n't\", 'wherever', 'would', 'through', 'both', 'out', 'next', 'make', 'she', 'full', 'not', 'due', 'six', 'us', 'down', 'neither', 'themselves', 'beyond', 'becomes', 'along', 'used', 'nine', 'thereupon', 'ever', 'around', 'seeming'}\n"
     ]
    }
   ],
   "source": [
    "# pip3 install spacy\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "#loading the english language small model of spacy\n",
    "nlp = English()\n",
    "\n",
    "# get th\n",
    "spacy_stop_words = nlp.Defaults.stop_words\n",
    "print(len(spacy_stop_words))\n",
    "print(spacy_stop_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also customize a list of stop words based on our own knowledge of the data. For example, in this case, the word coffee will not provide much information about the description since all the descriptions are talking about coffee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10      Crisply sweet-tart, richly fruit-toned Costa R...\n",
       "301     Evaluated as espresso. Extremely sweet, intent...\n",
       "531     Richly fruit-toned, complex. Dried mango, coco...\n",
       "819     A ready-to-drink black coffee, tested cold. Dr...\n",
       "820     A ready-to-drink black coffee, tested cold. Dr...\n",
       "                              ...                        \n",
       "2105    A ready-to-drink bottled black coffee tested o...\n",
       "2106    A ready-to-drink bottled black coffee tested o...\n",
       "2107    A ready-to-drink bottled black coffee tested o...\n",
       "2108    A ready-to-drink bottled black coffee tested o...\n",
       "2266    Evaluated as espresso. Bright, zesty-sweet. Ba...\n",
       "Name: desc_1, Length: 64, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find cases that contain the word coffee\n",
    "coffee.loc[coffee[\"desc_1\"].str.contains(\"coffee\") == True][\"desc_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = {\"coffee\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To combine our list of stop words to the list created by NLTK, use | which gives the union of two sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my', 'your', 'as', 'further', 'now', 'so', 'during', 'ours', 'yours', 'while', 'here', 'hers', 'didn', 'at', 'y', \"didn't\", \"couldn't\", 'own', 'herself', 'aren', 'i', 'be', \"don't\", 'myself', 'have', 'm', 'any', 'd', 'her', 'by', \"weren't\", 'how', \"you'd\", 'again', 'or', 'off', 'theirs', 'with', 'too', \"you've\", 'yourselves', 'needn', 'same', 'shouldn', 'are', 'of', 'you', 'o', 'hasn', 'over', 'up', 'each', 'been', 'a', 'and', 'our', 'most', 'if', 'nor', 've', \"haven't\", 'whom', 'did', \"won't\", 'more', 'than', 'such', 'had', 'were', 's', 'until', 'before', \"mustn't\", 'mustn', 'their', 'that', 'ourselves', \"should've\", 'don', \"aren't\", 'them', 'very', 'its', 'against', 'who', 'does', \"you'll\", 'into', 'being', 'below', 'in', \"it's\", \"needn't\", 're', \"you're\", 'ain', 'to', 'mightn', \"shan't\", 'is', \"doesn't\", 'we', 'his', 'then', 'isn', 'was', 'once', 'these', 'on', \"wouldn't\", 'himself', 'the', 'am', \"she's\", 'll', \"mightn't\", 'can', 'itself', 'shan', 'just', 'should', 'doesn', 'this', 'doing', 'wouldn', \"wasn't\", 'after', 'some', 'all', 'why', 'other', 'no', \"hadn't\", 'where', 't', 'because', \"isn't\", 'yourself', 'do', 'it', 'weren', 'what', 'when', \"hasn't\", 'through', 'ma', 'both', 'wasn', 'for', 'out', \"shouldn't\", 'above', 'she', 'there', 'not', 'only', 'having', 'an', 'which', 'down', 'themselves', 'between', 'will', 'from', 'has', 'under', 'haven', 'they', 'him', \"that'll\", 'couldn', 'won', 'few', 'those', 'but', 'me', 'he', 'about', 'coffee', 'hadn'}\n"
     ]
    }
   ],
   "source": [
    "combined_stop_words = stop_words | nltk_stop_words\n",
    "print(combined_stop_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use this to see how much overlap there is between spaCy's (326) and NLTK's (179) stop words. There are 382 unique words from combining both sets meaning, there's a 56 stop words in NLTK that don't appear in spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "382"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spacy_stop_words|nltk_stop_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a final list of stop words, we'll want to remove them from our tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>desc_1</th>\n",
       "      <th>lower_desc_1</th>\n",
       "      <th>clean_desc_1</th>\n",
       "      <th>desc_1_tokens</th>\n",
       "      <th>desc_1_tokens_nltk</th>\n",
       "      <th>desc_1_tokens_no_stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Richly floral-toned, exceptionally sweet. Dist...</td>\n",
       "      <td>richly floral-toned, exceptionally sweet. dist...</td>\n",
       "      <td>richly floral-toned exceptionally sweet distin...</td>\n",
       "      <td>[richly, floral-toned, exceptionally, sweet, d...</td>\n",
       "      <td>[richly, floral-toned, ,, exceptionally, sweet...</td>\n",
       "      <td>[richly, floral-toned, exceptionally, sweet, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Richly aromatic, chocolaty, fruit-toned. Dark ...</td>\n",
       "      <td>richly aromatic, chocolaty, fruit-toned. dark ...</td>\n",
       "      <td>richly aromatic chocolaty fruit-toned dark cho...</td>\n",
       "      <td>[richly, aromatic, chocolaty, fruit-toned, dar...</td>\n",
       "      <td>[richly, aromatic, ,, chocolaty, ,, fruit-tone...</td>\n",
       "      <td>[richly, aromatic, chocolaty, fruit-toned, dar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>High-toned, fruit-driven. Boysenberry, pear, c...</td>\n",
       "      <td>high-toned, fruit-driven. boysenberry, pear, c...</td>\n",
       "      <td>high-toned fruit-driven boysenberry pear cocoa...</td>\n",
       "      <td>[high-toned, fruit-driven, boysenberry, pear, ...</td>\n",
       "      <td>[high-toned, ,, fruit-driven, ., boysenberry, ...</td>\n",
       "      <td>[high-toned, fruit-driven, boysenberry, pear, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Delicately fruit-toned. Guava, ginger blossom,...</td>\n",
       "      <td>delicately fruit-toned. guava, ginger blossom,...</td>\n",
       "      <td>delicately fruit-toned guava ginger blossom co...</td>\n",
       "      <td>[delicately, fruit-toned, guava, ginger, bloss...</td>\n",
       "      <td>[delicately, fruit-toned, ., guava, ,, ginger,...</td>\n",
       "      <td>[delicately, fruit-toned, guava, ginger, bloss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Richly fruit-forward, floral-toned. Lychee, te...</td>\n",
       "      <td>richly fruit-forward, floral-toned. lychee, te...</td>\n",
       "      <td>richly fruit-forward floral-toned lychee tea r...</td>\n",
       "      <td>[richly, fruit-forward, floral-toned, lychee, ...</td>\n",
       "      <td>[richly, fruit-forward, ,, floral-toned, ., ly...</td>\n",
       "      <td>[richly, fruit-forward, floral-toned, lychee, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>High-toned, richly bittersweet. Pomelo, raspbe...</td>\n",
       "      <td>high-toned, richly bittersweet. pomelo, raspbe...</td>\n",
       "      <td>high-toned richly bittersweet pomelo raspberry...</td>\n",
       "      <td>[high-toned, richly, bittersweet, pomelo, rasp...</td>\n",
       "      <td>[high-toned, ,, richly, bittersweet, ., pomelo...</td>\n",
       "      <td>[high-toned, richly, bittersweet, pomelo, rasp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Crisply sweet-tart. Apricot, cocoa nib, agave ...</td>\n",
       "      <td>crisply sweet-tart. apricot, cocoa nib, agave ...</td>\n",
       "      <td>crisply sweet-tart apricot cocoa nib agave syr...</td>\n",
       "      <td>[crisply, sweet-tart, apricot, cocoa, nib, aga...</td>\n",
       "      <td>[crisply, sweet-tart, ., apricot, ,, cocoa, ni...</td>\n",
       "      <td>[crisply, sweet-tart, apricot, cocoa, nib, aga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>High-toned, juicy-sweet. Mango, cocoa nib, mag...</td>\n",
       "      <td>high-toned, juicy-sweet. mango, cocoa nib, mag...</td>\n",
       "      <td>high-toned juicy-sweet mango cocoa nib magnoli...</td>\n",
       "      <td>[high-toned, juicy-sweet, mango, cocoa, nib, m...</td>\n",
       "      <td>[high-toned, ,, juicy-sweet, ., mango, ,, coco...</td>\n",
       "      <td>[high-toned, juicy-sweet, mango, cocoa, nib, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Richly spice-toned, floral-driven. Bergamot, l...</td>\n",
       "      <td>richly spice-toned, floral-driven. bergamot, l...</td>\n",
       "      <td>richly spice-toned floral-driven bergamot lila...</td>\n",
       "      <td>[richly, spice-toned, floral-driven, bergamot,...</td>\n",
       "      <td>[richly, spice-toned, ,, floral-driven, ., ber...</td>\n",
       "      <td>[richly, spice-toned, floral-driven, bergamot,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>High-toned, crisply sweet-tart. Lemongrass, co...</td>\n",
       "      <td>high-toned, crisply sweet-tart. lemongrass, co...</td>\n",
       "      <td>high-toned crisply sweet-tart lemongrass cocoa...</td>\n",
       "      <td>[high-toned, crisply, sweet-tart, lemongrass, ...</td>\n",
       "      <td>[high-toned, ,, crisply, sweet-tart, ., lemong...</td>\n",
       "      <td>[high-toned, crisply, sweet-tart, lemongrass, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              desc_1  \\\n",
       "0  Richly floral-toned, exceptionally sweet. Dist...   \n",
       "1  Richly aromatic, chocolaty, fruit-toned. Dark ...   \n",
       "2  High-toned, fruit-driven. Boysenberry, pear, c...   \n",
       "3  Delicately fruit-toned. Guava, ginger blossom,...   \n",
       "4  Richly fruit-forward, floral-toned. Lychee, te...   \n",
       "5  High-toned, richly bittersweet. Pomelo, raspbe...   \n",
       "6  Crisply sweet-tart. Apricot, cocoa nib, agave ...   \n",
       "7  High-toned, juicy-sweet. Mango, cocoa nib, mag...   \n",
       "8  Richly spice-toned, floral-driven. Bergamot, l...   \n",
       "9  High-toned, crisply sweet-tart. Lemongrass, co...   \n",
       "\n",
       "                                        lower_desc_1  \\\n",
       "0  richly floral-toned, exceptionally sweet. dist...   \n",
       "1  richly aromatic, chocolaty, fruit-toned. dark ...   \n",
       "2  high-toned, fruit-driven. boysenberry, pear, c...   \n",
       "3  delicately fruit-toned. guava, ginger blossom,...   \n",
       "4  richly fruit-forward, floral-toned. lychee, te...   \n",
       "5  high-toned, richly bittersweet. pomelo, raspbe...   \n",
       "6  crisply sweet-tart. apricot, cocoa nib, agave ...   \n",
       "7  high-toned, juicy-sweet. mango, cocoa nib, mag...   \n",
       "8  richly spice-toned, floral-driven. bergamot, l...   \n",
       "9  high-toned, crisply sweet-tart. lemongrass, co...   \n",
       "\n",
       "                                        clean_desc_1  \\\n",
       "0  richly floral-toned exceptionally sweet distin...   \n",
       "1  richly aromatic chocolaty fruit-toned dark cho...   \n",
       "2  high-toned fruit-driven boysenberry pear cocoa...   \n",
       "3  delicately fruit-toned guava ginger blossom co...   \n",
       "4  richly fruit-forward floral-toned lychee tea r...   \n",
       "5  high-toned richly bittersweet pomelo raspberry...   \n",
       "6  crisply sweet-tart apricot cocoa nib agave syr...   \n",
       "7  high-toned juicy-sweet mango cocoa nib magnoli...   \n",
       "8  richly spice-toned floral-driven bergamot lila...   \n",
       "9  high-toned crisply sweet-tart lemongrass cocoa...   \n",
       "\n",
       "                                       desc_1_tokens  \\\n",
       "0  [richly, floral-toned, exceptionally, sweet, d...   \n",
       "1  [richly, aromatic, chocolaty, fruit-toned, dar...   \n",
       "2  [high-toned, fruit-driven, boysenberry, pear, ...   \n",
       "3  [delicately, fruit-toned, guava, ginger, bloss...   \n",
       "4  [richly, fruit-forward, floral-toned, lychee, ...   \n",
       "5  [high-toned, richly, bittersweet, pomelo, rasp...   \n",
       "6  [crisply, sweet-tart, apricot, cocoa, nib, aga...   \n",
       "7  [high-toned, juicy-sweet, mango, cocoa, nib, m...   \n",
       "8  [richly, spice-toned, floral-driven, bergamot,...   \n",
       "9  [high-toned, crisply, sweet-tart, lemongrass, ...   \n",
       "\n",
       "                                  desc_1_tokens_nltk  \\\n",
       "0  [richly, floral-toned, ,, exceptionally, sweet...   \n",
       "1  [richly, aromatic, ,, chocolaty, ,, fruit-tone...   \n",
       "2  [high-toned, ,, fruit-driven, ., boysenberry, ...   \n",
       "3  [delicately, fruit-toned, ., guava, ,, ginger,...   \n",
       "4  [richly, fruit-forward, ,, floral-toned, ., ly...   \n",
       "5  [high-toned, ,, richly, bittersweet, ., pomelo...   \n",
       "6  [crisply, sweet-tart, ., apricot, ,, cocoa, ni...   \n",
       "7  [high-toned, ,, juicy-sweet, ., mango, ,, coco...   \n",
       "8  [richly, spice-toned, ,, floral-driven, ., ber...   \n",
       "9  [high-toned, ,, crisply, sweet-tart, ., lemong...   \n",
       "\n",
       "                               desc_1_tokens_no_stop  \n",
       "0  [richly, floral-toned, exceptionally, sweet, d...  \n",
       "1  [richly, aromatic, chocolaty, fruit-toned, dar...  \n",
       "2  [high-toned, fruit-driven, boysenberry, pear, ...  \n",
       "3  [delicately, fruit-toned, guava, ginger, bloss...  \n",
       "4  [richly, fruit-forward, floral-toned, lychee, ...  \n",
       "5  [high-toned, richly, bittersweet, pomelo, rasp...  \n",
       "6  [crisply, sweet-tart, apricot, cocoa, nib, aga...  \n",
       "7  [high-toned, juicy-sweet, mango, cocoa, nib, m...  \n",
       "8  [richly, spice-toned, floral-driven, bergamot,...  \n",
       "9  [high-toned, crisply, sweet-tart, lemongrass, ...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a function with that requires 1 input, a list of tokens 1\n",
    "# initate a new list 2\n",
    "# for each token in the list 3\n",
    "# if the token is not in our list of stop words 4\n",
    "# append the word to the initated list 5\n",
    "# return the list of non-stop words 6\n",
    "\n",
    "def remove_stop_words(token_list): # 1\n",
    "    new_tokens = [] # 2\n",
    "    for token in token_list: # 3\n",
    "        if token not in combined_stop_words: # 4\n",
    "            new_tokens.append(token) # 5\n",
    "    return new_tokens # 6\n",
    "\n",
    "# create a new column in our data set of clean tokens with the stop words removed\n",
    "coffee[\"desc_1_tokens_no_stop\"] = coffee[\"desc_1_tokens\"].apply(remove_stop_words)\n",
    "\n",
    "# view the original desc_1 column, cleaned, and split side-by-side for the first 10 cases\n",
    "coffee[[\"desc_1\", \"lower_desc_1\", \"clean_desc_1\", \"desc_1_tokens\", \"desc_1_tokens_nltk\", \"desc_1_tokens_no_stop\"]][0:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many stop words were removed from each row..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    10\n",
      "1     6\n",
      "2     9\n",
      "3     6\n",
      "4     4\n",
      "5    12\n",
      "6     5\n",
      "7     6\n",
      "8     9\n",
      "9     9\n",
      "dtype: int64\n",
      "on avg there were 8.123 stop words removed\n"
     ]
    }
   ],
   "source": [
    "# number of stop words removed from each row\n",
    "stop_words_removed = coffee[\"desc_1_tokens\"].apply(len) - coffee[\"desc_1_tokens_no_stop\"].apply(len)\n",
    "print(stop_words_removed[0:10])\n",
    "\n",
    "# average number of stop words removed\n",
    "print(\"on avg there were\", round(stop_words_removed.mean(), 3), \"stop words removed\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "**Stemming** is the process of reducing a word to its root form so that a group of related words are captured in the same stem. For example, if we refer back to the course feedback example, \"book\" and \"books\" would be captured as separate tokens without stemming. After stemming, the root of both these words is \"book.\" Stemming applies a rule based approach for slicing the prefix and/or suffix from a word. \n",
    "\n",
    "However, stemming can result in incorrect grouping of words from over- and under-stemming.\n",
    "\n",
    "**Over-stemming** is when two words are stemmed to the same root but should be different roots. For example, if \"university\" and \"universe\" are both stemmed to \"univers.\"\n",
    "\n",
    "**Under-stemming** is when two words are stemmed to different roots but they should be the same root. For example, if \"fair\" and \"farily\" are separate roots. \n",
    "\n",
    "When stemming, the root may not neccesarily have an appropriate meaning, for example \"univers,\" but can still be useful for the purpose of the analysis.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One commonly used stemmer is the Porter stemmer which was written and is mantained by Martin Porter. The Porter stemmer removes commoner morphological and inflexional endings from words in English (Porter, 1980).\n",
    "- **Morphological:** the structure of words such as stems, root words, prefixes, and suffixes\n",
    "- **Inflectional:** changes in the form of a word to distinguish tense, person, number, gender, mood, voice, or case.\n",
    "\n",
    "There is also the Snowball stemmer which is a revised version of the Porter stemmer. And the Lancaster stemmer which is the most agressive of the three stemmers and often leads to over stemming. \n",
    "\n",
    "Regardless of the stemmer used, stemming does not consider how a word is being used in the context of the sentence e.g., if it's being used as a noun or a verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the stemmers from NLTK\n",
    "porter_stemmer = nltk.stem.porter.PorterStemmer()\n",
    "snowball_stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "lancaster_stemmer = nltk.stem.lancaster.LancasterStemmer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though each of the stemmers produce different stems, it is fine so long as they aren't over- or under-stemming. In the case below, the Snowball and Lancaster stemmers perform well while the Porter stem under-stems exceed and exceedingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "porter stem of exceed: exceed\n",
      "porter stem of exceedingly: exceedingli \n",
      "\n",
      "snowball stem of exceed: exceed\n",
      "snowball stem of exceedingly: exceed \n",
      "\n",
      "lancaster stem of exceed: excess\n",
      "lancaster stem of exceedingly: excess\n"
     ]
    }
   ],
   "source": [
    "# note differences in the stems from different stemmers\n",
    "print(\"porter stem of exceed:\", porter_stemmer.stem(\"exceed\"))\n",
    "print(\"porter stem of exceedingly:\", porter_stemmer.stem(\"exceedingly\"), \"\\n\")\n",
    "\n",
    "print(\"snowball stem of exceed:\", snowball_stemmer.stem(\"exceed\"))\n",
    "print(\"snowball stem of exceedingly:\", snowball_stemmer.stem(\"exceedingly\"), \"\\n\")\n",
    "\n",
    "print(\"lancaster stem of exceed:\", lancaster_stemmer.stem(\"exceed\"))\n",
    "print(\"lancaster stem of exceedingly:\", lancaster_stemmer.stem(\"exceedingly\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lancaster stemmer is the most agressive stemmer and can lead to over-stemming. We see this with \"mat\" and \"matter\" which should be two different stems like they are in the Porter and Snowball stemmers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "porter stem of mat: mat\n",
      "porter stem of matter: matter \n",
      "\n",
      "snowball stem of mat: mat\n",
      "snowball stem of matter: matter \n",
      "\n",
      "lancaster stem of mat: mat\n",
      "lancaster stem of matter: mat\n"
     ]
    }
   ],
   "source": [
    "# over-stemming\n",
    "print(\"porter stem of mat:\", porter_stemmer.stem(\"mat\"))\n",
    "print(\"porter stem of matter:\", porter_stemmer.stem(\"matter\"), \"\\n\")\n",
    "\n",
    "print(\"snowball stem of mat:\", snowball_stemmer.stem(\"mat\"))\n",
    "print(\"snowball stem of matter:\", snowball_stemmer.stem(\"matter\"), \"\\n\")\n",
    "\n",
    "print(\"lancaster stem of mat:\", lancaster_stemmer.stem(\"mat\"))\n",
    "print(\"lancaster stem of matter:\", lancaster_stemmer.stem(\"matter\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use the Snowball stemmer on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>desc_1</th>\n",
       "      <th>lower_desc_1</th>\n",
       "      <th>clean_desc_1</th>\n",
       "      <th>desc_1_tokens</th>\n",
       "      <th>desc_1_tokens_no_stop</th>\n",
       "      <th>desc_1_token_stems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Richly floral-toned, exceptionally sweet. Dist...</td>\n",
       "      <td>richly floral-toned, exceptionally sweet. dist...</td>\n",
       "      <td>richly floral-toned exceptionally sweet distin...</td>\n",
       "      <td>[richly, floral-toned, exceptionally, sweet, d...</td>\n",
       "      <td>[richly, floral-toned, exceptionally, sweet, d...</td>\n",
       "      <td>[rich, floral-ton, except, sweet, distinct, na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Richly aromatic, chocolaty, fruit-toned. Dark ...</td>\n",
       "      <td>richly aromatic, chocolaty, fruit-toned. dark ...</td>\n",
       "      <td>richly aromatic chocolaty fruit-toned dark cho...</td>\n",
       "      <td>[richly, aromatic, chocolaty, fruit-toned, dar...</td>\n",
       "      <td>[richly, aromatic, chocolaty, fruit-toned, dar...</td>\n",
       "      <td>[rich, aromat, chocolati, fruit-ton, dark, cho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>High-toned, fruit-driven. Boysenberry, pear, c...</td>\n",
       "      <td>high-toned, fruit-driven. boysenberry, pear, c...</td>\n",
       "      <td>high-toned fruit-driven boysenberry pear cocoa...</td>\n",
       "      <td>[high-toned, fruit-driven, boysenberry, pear, ...</td>\n",
       "      <td>[high-toned, fruit-driven, boysenberry, pear, ...</td>\n",
       "      <td>[high-ton, fruit-driven, boysenberri, pear, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Delicately fruit-toned. Guava, ginger blossom,...</td>\n",
       "      <td>delicately fruit-toned. guava, ginger blossom,...</td>\n",
       "      <td>delicately fruit-toned guava ginger blossom co...</td>\n",
       "      <td>[delicately, fruit-toned, guava, ginger, bloss...</td>\n",
       "      <td>[delicately, fruit-toned, guava, ginger, bloss...</td>\n",
       "      <td>[delic, fruit-ton, guava, ginger, blossom, coc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Richly fruit-forward, floral-toned. Lychee, te...</td>\n",
       "      <td>richly fruit-forward, floral-toned. lychee, te...</td>\n",
       "      <td>richly fruit-forward floral-toned lychee tea r...</td>\n",
       "      <td>[richly, fruit-forward, floral-toned, lychee, ...</td>\n",
       "      <td>[richly, fruit-forward, floral-toned, lychee, ...</td>\n",
       "      <td>[rich, fruit-forward, floral-ton, lyche, tea, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>High-toned, richly bittersweet. Pomelo, raspbe...</td>\n",
       "      <td>high-toned, richly bittersweet. pomelo, raspbe...</td>\n",
       "      <td>high-toned richly bittersweet pomelo raspberry...</td>\n",
       "      <td>[high-toned, richly, bittersweet, pomelo, rasp...</td>\n",
       "      <td>[high-toned, richly, bittersweet, pomelo, rasp...</td>\n",
       "      <td>[high-ton, rich, bittersweet, pomelo, raspberr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Crisply sweet-tart. Apricot, cocoa nib, agave ...</td>\n",
       "      <td>crisply sweet-tart. apricot, cocoa nib, agave ...</td>\n",
       "      <td>crisply sweet-tart apricot cocoa nib agave syr...</td>\n",
       "      <td>[crisply, sweet-tart, apricot, cocoa, nib, aga...</td>\n",
       "      <td>[crisply, sweet-tart, apricot, cocoa, nib, aga...</td>\n",
       "      <td>[crispli, sweet-tart, apricot, cocoa, nib, aga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>High-toned, juicy-sweet. Mango, cocoa nib, mag...</td>\n",
       "      <td>high-toned, juicy-sweet. mango, cocoa nib, mag...</td>\n",
       "      <td>high-toned juicy-sweet mango cocoa nib magnoli...</td>\n",
       "      <td>[high-toned, juicy-sweet, mango, cocoa, nib, m...</td>\n",
       "      <td>[high-toned, juicy-sweet, mango, cocoa, nib, m...</td>\n",
       "      <td>[high-ton, juicy-sweet, mango, cocoa, nib, mag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Richly spice-toned, floral-driven. Bergamot, l...</td>\n",
       "      <td>richly spice-toned, floral-driven. bergamot, l...</td>\n",
       "      <td>richly spice-toned floral-driven bergamot lila...</td>\n",
       "      <td>[richly, spice-toned, floral-driven, bergamot,...</td>\n",
       "      <td>[richly, spice-toned, floral-driven, bergamot,...</td>\n",
       "      <td>[rich, spice-ton, floral-driven, bergamot, lil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>High-toned, crisply sweet-tart. Lemongrass, co...</td>\n",
       "      <td>high-toned, crisply sweet-tart. lemongrass, co...</td>\n",
       "      <td>high-toned crisply sweet-tart lemongrass cocoa...</td>\n",
       "      <td>[high-toned, crisply, sweet-tart, lemongrass, ...</td>\n",
       "      <td>[high-toned, crisply, sweet-tart, lemongrass, ...</td>\n",
       "      <td>[high-ton, crispli, sweet-tart, lemongrass, co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              desc_1  \\\n",
       "0  Richly floral-toned, exceptionally sweet. Dist...   \n",
       "1  Richly aromatic, chocolaty, fruit-toned. Dark ...   \n",
       "2  High-toned, fruit-driven. Boysenberry, pear, c...   \n",
       "3  Delicately fruit-toned. Guava, ginger blossom,...   \n",
       "4  Richly fruit-forward, floral-toned. Lychee, te...   \n",
       "5  High-toned, richly bittersweet. Pomelo, raspbe...   \n",
       "6  Crisply sweet-tart. Apricot, cocoa nib, agave ...   \n",
       "7  High-toned, juicy-sweet. Mango, cocoa nib, mag...   \n",
       "8  Richly spice-toned, floral-driven. Bergamot, l...   \n",
       "9  High-toned, crisply sweet-tart. Lemongrass, co...   \n",
       "\n",
       "                                        lower_desc_1  \\\n",
       "0  richly floral-toned, exceptionally sweet. dist...   \n",
       "1  richly aromatic, chocolaty, fruit-toned. dark ...   \n",
       "2  high-toned, fruit-driven. boysenberry, pear, c...   \n",
       "3  delicately fruit-toned. guava, ginger blossom,...   \n",
       "4  richly fruit-forward, floral-toned. lychee, te...   \n",
       "5  high-toned, richly bittersweet. pomelo, raspbe...   \n",
       "6  crisply sweet-tart. apricot, cocoa nib, agave ...   \n",
       "7  high-toned, juicy-sweet. mango, cocoa nib, mag...   \n",
       "8  richly spice-toned, floral-driven. bergamot, l...   \n",
       "9  high-toned, crisply sweet-tart. lemongrass, co...   \n",
       "\n",
       "                                        clean_desc_1  \\\n",
       "0  richly floral-toned exceptionally sweet distin...   \n",
       "1  richly aromatic chocolaty fruit-toned dark cho...   \n",
       "2  high-toned fruit-driven boysenberry pear cocoa...   \n",
       "3  delicately fruit-toned guava ginger blossom co...   \n",
       "4  richly fruit-forward floral-toned lychee tea r...   \n",
       "5  high-toned richly bittersweet pomelo raspberry...   \n",
       "6  crisply sweet-tart apricot cocoa nib agave syr...   \n",
       "7  high-toned juicy-sweet mango cocoa nib magnoli...   \n",
       "8  richly spice-toned floral-driven bergamot lila...   \n",
       "9  high-toned crisply sweet-tart lemongrass cocoa...   \n",
       "\n",
       "                                       desc_1_tokens  \\\n",
       "0  [richly, floral-toned, exceptionally, sweet, d...   \n",
       "1  [richly, aromatic, chocolaty, fruit-toned, dar...   \n",
       "2  [high-toned, fruit-driven, boysenberry, pear, ...   \n",
       "3  [delicately, fruit-toned, guava, ginger, bloss...   \n",
       "4  [richly, fruit-forward, floral-toned, lychee, ...   \n",
       "5  [high-toned, richly, bittersweet, pomelo, rasp...   \n",
       "6  [crisply, sweet-tart, apricot, cocoa, nib, aga...   \n",
       "7  [high-toned, juicy-sweet, mango, cocoa, nib, m...   \n",
       "8  [richly, spice-toned, floral-driven, bergamot,...   \n",
       "9  [high-toned, crisply, sweet-tart, lemongrass, ...   \n",
       "\n",
       "                               desc_1_tokens_no_stop  \\\n",
       "0  [richly, floral-toned, exceptionally, sweet, d...   \n",
       "1  [richly, aromatic, chocolaty, fruit-toned, dar...   \n",
       "2  [high-toned, fruit-driven, boysenberry, pear, ...   \n",
       "3  [delicately, fruit-toned, guava, ginger, bloss...   \n",
       "4  [richly, fruit-forward, floral-toned, lychee, ...   \n",
       "5  [high-toned, richly, bittersweet, pomelo, rasp...   \n",
       "6  [crisply, sweet-tart, apricot, cocoa, nib, aga...   \n",
       "7  [high-toned, juicy-sweet, mango, cocoa, nib, m...   \n",
       "8  [richly, spice-toned, floral-driven, bergamot,...   \n",
       "9  [high-toned, crisply, sweet-tart, lemongrass, ...   \n",
       "\n",
       "                                  desc_1_token_stems  \n",
       "0  [rich, floral-ton, except, sweet, distinct, na...  \n",
       "1  [rich, aromat, chocolati, fruit-ton, dark, cho...  \n",
       "2  [high-ton, fruit-driven, boysenberri, pear, co...  \n",
       "3  [delic, fruit-ton, guava, ginger, blossom, coc...  \n",
       "4  [rich, fruit-forward, floral-ton, lyche, tea, ...  \n",
       "5  [high-ton, rich, bittersweet, pomelo, raspberr...  \n",
       "6  [crispli, sweet-tart, apricot, cocoa, nib, aga...  \n",
       "7  [high-ton, juicy-sweet, mango, cocoa, nib, mag...  \n",
       "8  [rich, spice-ton, floral-driven, bergamot, lil...  \n",
       "9  [high-ton, crispli, sweet-tart, lemongrass, co...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a function with that requires 1 input, a list of tokens 1\n",
    "# initate a new list 2\n",
    "# for each token in the list 3\n",
    "# use the stemmer to create a stemmed word 4\n",
    "# append the word to the initated list 5\n",
    "# return the list of stemmed tokens 6\n",
    "\n",
    "def stem_tokens(token_list): # 1\n",
    "    stemmed_tokens = [] # 2\n",
    "    for token in token_list: # 3\n",
    "        stem = snowball_stemmer.stem(token) # 4\n",
    "        stemmed_tokens.append(stem) # 5\n",
    "    return stemmed_tokens # 6\n",
    "\n",
    "# create a new column in our data set of clean tokens with the stop words removed\n",
    "coffee[\"desc_1_token_stems\"] = coffee[\"desc_1_tokens_no_stop\"].apply(stem_tokens)\n",
    "\n",
    "# view the original desc_1 column, cleaned, and split side-by-side for the first 10 cases\n",
    "coffee[[\"desc_1\", \"lower_desc_1\", \"clean_desc_1\", \"desc_1_tokens\",\"desc_1_tokens_no_stop\", \"desc_1_token_stems\"]][0:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "**Lemmatization** attempts to find the lemma or dictionary form of a word. This takes into consideration the way it was used in the sentence which typically means it removes inflectional endings only. \n",
    "\n",
    "Though more nuanced than stemming, lemmatizers are also more computationally intensive and are more laborious to develop because they require a deeper understanding of the language. \n",
    "\n",
    "NLTK's lemmatizer is based off of the WordNet lexical database (https://wordnet.princeton.edu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initalize the lemmatizer\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snowball stem of study: studi\n",
      "snowball stem of studies: studi\n",
      "snowball stem of studying: studi\n",
      "snowball stem of studied: studi \n",
      "\n",
      "lemma of study: study\n",
      "lemma of studies: study\n",
      "lemma of studying: studying\n",
      "lemma of studied: studied\n"
     ]
    }
   ],
   "source": [
    "# compare the stemmer and lemmatizer\n",
    "print(\"snowball stem of study:\", snowball_stemmer.stem(\"study\"))\n",
    "print(\"snowball stem of studies:\", snowball_stemmer.stem(\"studies\"))\n",
    "print(\"snowball stem of studying:\", snowball_stemmer.stem(\"studying\"))\n",
    "print(\"snowball stem of studied:\", snowball_stemmer.stem(\"studied\"), \"\\n\")\n",
    "\n",
    "print(\"lemma of study:\", lemmatizer.lemmatize(\"study\"))\n",
    "print(\"lemma of studies:\", lemmatizer.lemmatize(\"studies\"))\n",
    "print(\"lemma of studying:\", lemmatizer.lemmatize(\"studying\"))\n",
    "print(\"lemma of studied:\", lemmatizer.lemmatize(\"studied\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>desc_1</th>\n",
       "      <th>lower_desc_1</th>\n",
       "      <th>clean_desc_1</th>\n",
       "      <th>desc_1_tokens</th>\n",
       "      <th>desc_1_tokens_no_stop</th>\n",
       "      <th>desc_1_token_stems</th>\n",
       "      <th>desc_1_token_lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Richly floral-toned, exceptionally sweet. Dist...</td>\n",
       "      <td>richly floral-toned, exceptionally sweet. dist...</td>\n",
       "      <td>richly floral-toned exceptionally sweet distin...</td>\n",
       "      <td>[richly, floral-toned, exceptionally, sweet, d...</td>\n",
       "      <td>[richly, floral-toned, exceptionally, sweet, d...</td>\n",
       "      <td>[rich, floral-ton, except, sweet, distinct, na...</td>\n",
       "      <td>[richly, floral-toned, exceptionally, sweet, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Richly aromatic, chocolaty, fruit-toned. Dark ...</td>\n",
       "      <td>richly aromatic, chocolaty, fruit-toned. dark ...</td>\n",
       "      <td>richly aromatic chocolaty fruit-toned dark cho...</td>\n",
       "      <td>[richly, aromatic, chocolaty, fruit-toned, dar...</td>\n",
       "      <td>[richly, aromatic, chocolaty, fruit-toned, dar...</td>\n",
       "      <td>[rich, aromat, chocolati, fruit-ton, dark, cho...</td>\n",
       "      <td>[richly, aromatic, chocolaty, fruit-toned, dar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>High-toned, fruit-driven. Boysenberry, pear, c...</td>\n",
       "      <td>high-toned, fruit-driven. boysenberry, pear, c...</td>\n",
       "      <td>high-toned fruit-driven boysenberry pear cocoa...</td>\n",
       "      <td>[high-toned, fruit-driven, boysenberry, pear, ...</td>\n",
       "      <td>[high-toned, fruit-driven, boysenberry, pear, ...</td>\n",
       "      <td>[high-ton, fruit-driven, boysenberri, pear, co...</td>\n",
       "      <td>[high-toned, fruit-driven, boysenberry, pear, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Delicately fruit-toned. Guava, ginger blossom,...</td>\n",
       "      <td>delicately fruit-toned. guava, ginger blossom,...</td>\n",
       "      <td>delicately fruit-toned guava ginger blossom co...</td>\n",
       "      <td>[delicately, fruit-toned, guava, ginger, bloss...</td>\n",
       "      <td>[delicately, fruit-toned, guava, ginger, bloss...</td>\n",
       "      <td>[delic, fruit-ton, guava, ginger, blossom, coc...</td>\n",
       "      <td>[delicately, fruit-toned, guava, ginger, bloss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Richly fruit-forward, floral-toned. Lychee, te...</td>\n",
       "      <td>richly fruit-forward, floral-toned. lychee, te...</td>\n",
       "      <td>richly fruit-forward floral-toned lychee tea r...</td>\n",
       "      <td>[richly, fruit-forward, floral-toned, lychee, ...</td>\n",
       "      <td>[richly, fruit-forward, floral-toned, lychee, ...</td>\n",
       "      <td>[rich, fruit-forward, floral-ton, lyche, tea, ...</td>\n",
       "      <td>[richly, fruit-forward, floral-toned, lychee, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>High-toned, richly bittersweet. Pomelo, raspbe...</td>\n",
       "      <td>high-toned, richly bittersweet. pomelo, raspbe...</td>\n",
       "      <td>high-toned richly bittersweet pomelo raspberry...</td>\n",
       "      <td>[high-toned, richly, bittersweet, pomelo, rasp...</td>\n",
       "      <td>[high-toned, richly, bittersweet, pomelo, rasp...</td>\n",
       "      <td>[high-ton, rich, bittersweet, pomelo, raspberr...</td>\n",
       "      <td>[high-toned, richly, bittersweet, pomelo, rasp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Crisply sweet-tart. Apricot, cocoa nib, agave ...</td>\n",
       "      <td>crisply sweet-tart. apricot, cocoa nib, agave ...</td>\n",
       "      <td>crisply sweet-tart apricot cocoa nib agave syr...</td>\n",
       "      <td>[crisply, sweet-tart, apricot, cocoa, nib, aga...</td>\n",
       "      <td>[crisply, sweet-tart, apricot, cocoa, nib, aga...</td>\n",
       "      <td>[crispli, sweet-tart, apricot, cocoa, nib, aga...</td>\n",
       "      <td>[crisply, sweet-tart, apricot, cocoa, nib, aga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>High-toned, juicy-sweet. Mango, cocoa nib, mag...</td>\n",
       "      <td>high-toned, juicy-sweet. mango, cocoa nib, mag...</td>\n",
       "      <td>high-toned juicy-sweet mango cocoa nib magnoli...</td>\n",
       "      <td>[high-toned, juicy-sweet, mango, cocoa, nib, m...</td>\n",
       "      <td>[high-toned, juicy-sweet, mango, cocoa, nib, m...</td>\n",
       "      <td>[high-ton, juicy-sweet, mango, cocoa, nib, mag...</td>\n",
       "      <td>[high-toned, juicy-sweet, mango, cocoa, nib, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Richly spice-toned, floral-driven. Bergamot, l...</td>\n",
       "      <td>richly spice-toned, floral-driven. bergamot, l...</td>\n",
       "      <td>richly spice-toned floral-driven bergamot lila...</td>\n",
       "      <td>[richly, spice-toned, floral-driven, bergamot,...</td>\n",
       "      <td>[richly, spice-toned, floral-driven, bergamot,...</td>\n",
       "      <td>[rich, spice-ton, floral-driven, bergamot, lil...</td>\n",
       "      <td>[richly, spice-toned, floral-driven, bergamot,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>High-toned, crisply sweet-tart. Lemongrass, co...</td>\n",
       "      <td>high-toned, crisply sweet-tart. lemongrass, co...</td>\n",
       "      <td>high-toned crisply sweet-tart lemongrass cocoa...</td>\n",
       "      <td>[high-toned, crisply, sweet-tart, lemongrass, ...</td>\n",
       "      <td>[high-toned, crisply, sweet-tart, lemongrass, ...</td>\n",
       "      <td>[high-ton, crispli, sweet-tart, lemongrass, co...</td>\n",
       "      <td>[high-toned, crisply, sweet-tart, lemongrass, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              desc_1  \\\n",
       "0  Richly floral-toned, exceptionally sweet. Dist...   \n",
       "1  Richly aromatic, chocolaty, fruit-toned. Dark ...   \n",
       "2  High-toned, fruit-driven. Boysenberry, pear, c...   \n",
       "3  Delicately fruit-toned. Guava, ginger blossom,...   \n",
       "4  Richly fruit-forward, floral-toned. Lychee, te...   \n",
       "5  High-toned, richly bittersweet. Pomelo, raspbe...   \n",
       "6  Crisply sweet-tart. Apricot, cocoa nib, agave ...   \n",
       "7  High-toned, juicy-sweet. Mango, cocoa nib, mag...   \n",
       "8  Richly spice-toned, floral-driven. Bergamot, l...   \n",
       "9  High-toned, crisply sweet-tart. Lemongrass, co...   \n",
       "\n",
       "                                        lower_desc_1  \\\n",
       "0  richly floral-toned, exceptionally sweet. dist...   \n",
       "1  richly aromatic, chocolaty, fruit-toned. dark ...   \n",
       "2  high-toned, fruit-driven. boysenberry, pear, c...   \n",
       "3  delicately fruit-toned. guava, ginger blossom,...   \n",
       "4  richly fruit-forward, floral-toned. lychee, te...   \n",
       "5  high-toned, richly bittersweet. pomelo, raspbe...   \n",
       "6  crisply sweet-tart. apricot, cocoa nib, agave ...   \n",
       "7  high-toned, juicy-sweet. mango, cocoa nib, mag...   \n",
       "8  richly spice-toned, floral-driven. bergamot, l...   \n",
       "9  high-toned, crisply sweet-tart. lemongrass, co...   \n",
       "\n",
       "                                        clean_desc_1  \\\n",
       "0  richly floral-toned exceptionally sweet distin...   \n",
       "1  richly aromatic chocolaty fruit-toned dark cho...   \n",
       "2  high-toned fruit-driven boysenberry pear cocoa...   \n",
       "3  delicately fruit-toned guava ginger blossom co...   \n",
       "4  richly fruit-forward floral-toned lychee tea r...   \n",
       "5  high-toned richly bittersweet pomelo raspberry...   \n",
       "6  crisply sweet-tart apricot cocoa nib agave syr...   \n",
       "7  high-toned juicy-sweet mango cocoa nib magnoli...   \n",
       "8  richly spice-toned floral-driven bergamot lila...   \n",
       "9  high-toned crisply sweet-tart lemongrass cocoa...   \n",
       "\n",
       "                                       desc_1_tokens  \\\n",
       "0  [richly, floral-toned, exceptionally, sweet, d...   \n",
       "1  [richly, aromatic, chocolaty, fruit-toned, dar...   \n",
       "2  [high-toned, fruit-driven, boysenberry, pear, ...   \n",
       "3  [delicately, fruit-toned, guava, ginger, bloss...   \n",
       "4  [richly, fruit-forward, floral-toned, lychee, ...   \n",
       "5  [high-toned, richly, bittersweet, pomelo, rasp...   \n",
       "6  [crisply, sweet-tart, apricot, cocoa, nib, aga...   \n",
       "7  [high-toned, juicy-sweet, mango, cocoa, nib, m...   \n",
       "8  [richly, spice-toned, floral-driven, bergamot,...   \n",
       "9  [high-toned, crisply, sweet-tart, lemongrass, ...   \n",
       "\n",
       "                               desc_1_tokens_no_stop  \\\n",
       "0  [richly, floral-toned, exceptionally, sweet, d...   \n",
       "1  [richly, aromatic, chocolaty, fruit-toned, dar...   \n",
       "2  [high-toned, fruit-driven, boysenberry, pear, ...   \n",
       "3  [delicately, fruit-toned, guava, ginger, bloss...   \n",
       "4  [richly, fruit-forward, floral-toned, lychee, ...   \n",
       "5  [high-toned, richly, bittersweet, pomelo, rasp...   \n",
       "6  [crisply, sweet-tart, apricot, cocoa, nib, aga...   \n",
       "7  [high-toned, juicy-sweet, mango, cocoa, nib, m...   \n",
       "8  [richly, spice-toned, floral-driven, bergamot,...   \n",
       "9  [high-toned, crisply, sweet-tart, lemongrass, ...   \n",
       "\n",
       "                                  desc_1_token_stems  \\\n",
       "0  [rich, floral-ton, except, sweet, distinct, na...   \n",
       "1  [rich, aromat, chocolati, fruit-ton, dark, cho...   \n",
       "2  [high-ton, fruit-driven, boysenberri, pear, co...   \n",
       "3  [delic, fruit-ton, guava, ginger, blossom, coc...   \n",
       "4  [rich, fruit-forward, floral-ton, lyche, tea, ...   \n",
       "5  [high-ton, rich, bittersweet, pomelo, raspberr...   \n",
       "6  [crispli, sweet-tart, apricot, cocoa, nib, aga...   \n",
       "7  [high-ton, juicy-sweet, mango, cocoa, nib, mag...   \n",
       "8  [rich, spice-ton, floral-driven, bergamot, lil...   \n",
       "9  [high-ton, crispli, sweet-tart, lemongrass, co...   \n",
       "\n",
       "                                 desc_1_token_lemmas  \n",
       "0  [richly, floral-toned, exceptionally, sweet, d...  \n",
       "1  [richly, aromatic, chocolaty, fruit-toned, dar...  \n",
       "2  [high-toned, fruit-driven, boysenberry, pear, ...  \n",
       "3  [delicately, fruit-toned, guava, ginger, bloss...  \n",
       "4  [richly, fruit-forward, floral-toned, lychee, ...  \n",
       "5  [high-toned, richly, bittersweet, pomelo, rasp...  \n",
       "6  [crisply, sweet-tart, apricot, cocoa, nib, aga...  \n",
       "7  [high-toned, juicy-sweet, mango, cocoa, nib, m...  \n",
       "8  [richly, spice-toned, floral-driven, bergamot,...  \n",
       "9  [high-toned, crisply, sweet-tart, lemongrass, ...  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a function that requires 1 input, a list of tokens 1\n",
    "# initate a new list 2\n",
    "# for each token in the list 3\n",
    "# use the lemmatizer to create lemmas from tokens 4\n",
    "# append the lemma to the initated list 5\n",
    "# return the list of lemmatized tokens 6\n",
    "\n",
    "def lemmatize_tokens(token_list): # 1\n",
    "    lemmatized_tokens = [] # 2\n",
    "    for token in token_list: # 3\n",
    "        lemma = lemmatizer.lemmatize(token) # 4\n",
    "        lemmatized_tokens.append(lemma) # 5\n",
    "    return lemmatized_tokens # 6\n",
    "\n",
    "# create a new column in our data set of clean tokens with the stop words removed\n",
    "coffee[\"desc_1_token_lemmas\"] = coffee[\"desc_1_tokens_no_stop\"].apply(lemmatize_tokens)\n",
    "\n",
    "# view the original desc_1 column, cleaned, and split side-by-side for the first 10 cases\n",
    "coffee[[\"desc_1\", \"lower_desc_1\", \"clean_desc_1\", \"desc_1_tokens\",\"desc_1_tokens_no_stop\", \"desc_1_token_stems\", \"desc_1_token_lemmas\"]][0:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this analysis, we're not concerned with differentiating parts of speech. Especially, given that these text are meant to be reviews of coffee, the part of speech is not relevant to the flavor profile. So, we'll move forward with the Snowball stems instead of lemmas. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's save our processed text to a new column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>desc_1</th>\n",
       "      <th>clean_desc_1</th>\n",
       "      <th>desc_1_token_stems</th>\n",
       "      <th>desc_1_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Richly floral-toned, exceptionally sweet. Dist...</td>\n",
       "      <td>richly floral-toned exceptionally sweet distin...</td>\n",
       "      <td>[rich, floral-ton, except, sweet, distinct, na...</td>\n",
       "      <td>rich floral-ton except sweet distinct narcissu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Richly aromatic, chocolaty, fruit-toned. Dark ...</td>\n",
       "      <td>richly aromatic chocolaty fruit-toned dark cho...</td>\n",
       "      <td>[rich, aromat, chocolati, fruit-ton, dark, cho...</td>\n",
       "      <td>rich aromat chocolati fruit-ton dark chocol dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>High-toned, fruit-driven. Boysenberry, pear, c...</td>\n",
       "      <td>high-toned fruit-driven boysenberry pear cocoa...</td>\n",
       "      <td>[high-ton, fruit-driven, boysenberri, pear, co...</td>\n",
       "      <td>high-ton fruit-driven boysenberri pear cocoa n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Delicately fruit-toned. Guava, ginger blossom,...</td>\n",
       "      <td>delicately fruit-toned guava ginger blossom co...</td>\n",
       "      <td>[delic, fruit-ton, guava, ginger, blossom, coc...</td>\n",
       "      <td>delic fruit-ton guava ginger blossom cocoa nib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Richly fruit-forward, floral-toned. Lychee, te...</td>\n",
       "      <td>richly fruit-forward floral-toned lychee tea r...</td>\n",
       "      <td>[rich, fruit-forward, floral-ton, lyche, tea, ...</td>\n",
       "      <td>rich fruit-forward floral-ton lyche tea rose d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>High-toned, richly bittersweet. Pomelo, raspbe...</td>\n",
       "      <td>high-toned richly bittersweet pomelo raspberry...</td>\n",
       "      <td>[high-ton, rich, bittersweet, pomelo, raspberr...</td>\n",
       "      <td>high-ton rich bittersweet pomelo raspberri coc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Crisply sweet-tart. Apricot, cocoa nib, agave ...</td>\n",
       "      <td>crisply sweet-tart apricot cocoa nib agave syr...</td>\n",
       "      <td>[crispli, sweet-tart, apricot, cocoa, nib, aga...</td>\n",
       "      <td>crispli sweet-tart apricot cocoa nib agav syru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>High-toned, juicy-sweet. Mango, cocoa nib, mag...</td>\n",
       "      <td>high-toned juicy-sweet mango cocoa nib magnoli...</td>\n",
       "      <td>[high-ton, juicy-sweet, mango, cocoa, nib, mag...</td>\n",
       "      <td>high-ton juicy-sweet mango cocoa nib magnolia ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Richly spice-toned, floral-driven. Bergamot, l...</td>\n",
       "      <td>richly spice-toned floral-driven bergamot lila...</td>\n",
       "      <td>[rich, spice-ton, floral-driven, bergamot, lil...</td>\n",
       "      <td>rich spice-ton floral-driven bergamot lilac co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>High-toned, crisply sweet-tart. Lemongrass, co...</td>\n",
       "      <td>high-toned crisply sweet-tart lemongrass cocoa...</td>\n",
       "      <td>[high-ton, crispli, sweet-tart, lemongrass, co...</td>\n",
       "      <td>high-ton crispli sweet-tart lemongrass cocoa n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              desc_1  \\\n",
       "0  Richly floral-toned, exceptionally sweet. Dist...   \n",
       "1  Richly aromatic, chocolaty, fruit-toned. Dark ...   \n",
       "2  High-toned, fruit-driven. Boysenberry, pear, c...   \n",
       "3  Delicately fruit-toned. Guava, ginger blossom,...   \n",
       "4  Richly fruit-forward, floral-toned. Lychee, te...   \n",
       "5  High-toned, richly bittersweet. Pomelo, raspbe...   \n",
       "6  Crisply sweet-tart. Apricot, cocoa nib, agave ...   \n",
       "7  High-toned, juicy-sweet. Mango, cocoa nib, mag...   \n",
       "8  Richly spice-toned, floral-driven. Bergamot, l...   \n",
       "9  High-toned, crisply sweet-tart. Lemongrass, co...   \n",
       "\n",
       "                                        clean_desc_1  \\\n",
       "0  richly floral-toned exceptionally sweet distin...   \n",
       "1  richly aromatic chocolaty fruit-toned dark cho...   \n",
       "2  high-toned fruit-driven boysenberry pear cocoa...   \n",
       "3  delicately fruit-toned guava ginger blossom co...   \n",
       "4  richly fruit-forward floral-toned lychee tea r...   \n",
       "5  high-toned richly bittersweet pomelo raspberry...   \n",
       "6  crisply sweet-tart apricot cocoa nib agave syr...   \n",
       "7  high-toned juicy-sweet mango cocoa nib magnoli...   \n",
       "8  richly spice-toned floral-driven bergamot lila...   \n",
       "9  high-toned crisply sweet-tart lemongrass cocoa...   \n",
       "\n",
       "                                  desc_1_token_stems  \\\n",
       "0  [rich, floral-ton, except, sweet, distinct, na...   \n",
       "1  [rich, aromat, chocolati, fruit-ton, dark, cho...   \n",
       "2  [high-ton, fruit-driven, boysenberri, pear, co...   \n",
       "3  [delic, fruit-ton, guava, ginger, blossom, coc...   \n",
       "4  [rich, fruit-forward, floral-ton, lyche, tea, ...   \n",
       "5  [high-ton, rich, bittersweet, pomelo, raspberr...   \n",
       "6  [crispli, sweet-tart, apricot, cocoa, nib, aga...   \n",
       "7  [high-ton, juicy-sweet, mango, cocoa, nib, mag...   \n",
       "8  [rich, spice-ton, floral-driven, bergamot, lil...   \n",
       "9  [high-ton, crispli, sweet-tart, lemongrass, co...   \n",
       "\n",
       "                                    desc_1_processed  \n",
       "0  rich floral-ton except sweet distinct narcissu...  \n",
       "1  rich aromat chocolati fruit-ton dark chocol dr...  \n",
       "2  high-ton fruit-driven boysenberri pear cocoa n...  \n",
       "3  delic fruit-ton guava ginger blossom cocoa nib...  \n",
       "4  rich fruit-forward floral-ton lyche tea rose d...  \n",
       "5  high-ton rich bittersweet pomelo raspberri coc...  \n",
       "6  crispli sweet-tart apricot cocoa nib agav syru...  \n",
       "7  high-ton juicy-sweet mango cocoa nib magnolia ...  \n",
       "8  rich spice-ton floral-driven bergamot lilac co...  \n",
       "9  high-ton crispli sweet-tart lemongrass cocoa n...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a function with that requires 1 input, a list of tokens 1\n",
    "# combine the list of tokens into a single string 2\n",
    "# return the string 3\n",
    "\n",
    "def combine_tokens(token_list):\n",
    "    new_string = \" \".join(token_list)\n",
    "    return new_string\n",
    "\n",
    "# create a new column in our data set of combined tokens\n",
    "coffee[\"desc_1_processed\"] = coffee[\"desc_1_token_stems\"].apply(combine_tokens)\n",
    "\n",
    "# view the columns side-by-side for the first 10 cases\n",
    "coffee[[\"desc_1\", \"clean_desc_1\", \"desc_1_token_stems\", \"desc_1_processed\"]][0:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a document-term matrix\n",
    "In the **document-term matrix (DTM)**, rows are comprised of documents, columns are comprised of terms (i.e., tokens), and cells become a count of each token in the document. We'll first use the CountVectorizer which converts a column of text into a document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2282x1281 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 64406 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initaialize the vectorizer\n",
    "vec = CountVectorizer()\n",
    "\n",
    "# use the vectorizer to create a dtm called X\n",
    "X = vec.fit_transform(coffee[\"desc_1_processed\"])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>access</th>\n",
       "      <th>accompani</th>\n",
       "      <th>acditi</th>\n",
       "      <th>acid</th>\n",
       "      <th>acidi</th>\n",
       "      <th>acidti</th>\n",
       "      <th>acorn</th>\n",
       "      <th>acrid</th>\n",
       "      <th>ad</th>\n",
       "      <th>add</th>\n",
       "      <th>...</th>\n",
       "      <th>yeasti</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yet</th>\n",
       "      <th>yogurt</th>\n",
       "      <th>yogurti</th>\n",
       "      <th>young</th>\n",
       "      <th>yuzu</th>\n",
       "      <th>zest</th>\n",
       "      <th>zesti</th>\n",
       "      <th>zesty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2277</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2278</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2279</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2280</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2281</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2282 rows × 1281 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      access  accompani  acditi  acid  acidi  acidti  acorn  acrid  ad  add  \\\n",
       "0          0          0       0     1      0       0      0      0   0    0   \n",
       "1          0          0       0     1      0       0      0      0   0    0   \n",
       "2          0          0       0     1      0       0      0      0   0    0   \n",
       "3          0          0       0     1      0       0      0      0   0    0   \n",
       "4          0          0       0     1      0       0      0      0   0    0   \n",
       "...      ...        ...     ...   ...    ...     ...    ...    ...  ..  ...   \n",
       "2277       0          0       0     0      0       0      0      0   0    0   \n",
       "2278       0          0       0     1      0       0      0      0   0    0   \n",
       "2279       0          0       0     1      0       0      0      0   0    0   \n",
       "2280       0          0       0     1      0       0      0      0   0    0   \n",
       "2281       0          0       0     0      0       0      0      0   0    0   \n",
       "\n",
       "      ...  yeasti  yellow  yet  yogurt  yogurti  young  yuzu  zest  zesti  \\\n",
       "0     ...       0       0    0       0        0      0     0     0      0   \n",
       "1     ...       0       0    0       0        0      0     0     0      0   \n",
       "2     ...       0       0    0       0        0      0     0     2      0   \n",
       "3     ...       0       0    0       0        0      0     0     0      0   \n",
       "4     ...       0       0    0       0        0      0     0     1      0   \n",
       "...   ...     ...     ...  ...     ...      ...    ...   ...   ...    ...   \n",
       "2277  ...       0       0    0       0        0      0     0     0      0   \n",
       "2278  ...       0       0    0       0        0      0     0     0      0   \n",
       "2279  ...       0       0    0       0        0      0     0     0      0   \n",
       "2280  ...       0       0    0       0        0      0     0     2      0   \n",
       "2281  ...       0       0    0       0        0      0     0     0      0   \n",
       "\n",
       "      zesty  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "...     ...  \n",
       "2277      0  \n",
       "2278      0  \n",
       "2279      0  \n",
       "2280      0  \n",
       "2281      0  \n",
       "\n",
       "[2282 rows x 1281 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this code extracts the index (i.e., row/document names) from coffee and applies them to our matrix, X\n",
    "# we also convert this matrix to a DataFrame\n",
    "df = pd.DataFrame(X.toarray(), columns = vec.get_feature_names_out(), index = coffee.index)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing word counts and sentiment\n",
    "\n",
    "There are some flavor defects that can be detected in coffee which result from over- or under-extracting, over- or under-roasting, etc. The list below is a collection of key words that are associated with flavor defects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of flavor defect key words\n",
    "flavor_defects = [\n",
    "    \"rotten\",\n",
    "    \"must\",\n",
    "    \"mold\",\n",
    "    \"potato\",\n",
    "    \"bitter\",\n",
    "    \"cappy\",\n",
    "    \"baggy\",\n",
    "    \"oat\",\n",
    "    \"grain\",\n",
    "    \"grass\",\n",
    "    \"hay\",\n",
    "    \"ash\",\n",
    "    \"carbon\",\n",
    "    \"burnt\",\n",
    "    \"scorched\",\n",
    "    \"sour\"]\n",
    "\n",
    "# use the function we created to stem these key words\n",
    "flavor_defect_stems = stem_tokens(flavor_defects)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Were any of these key words detected in our document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rotten', 'bitter', 'grain', 'grass', 'sour']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initate a new list 1\n",
    "# for each word in our list of flavor defects 2\n",
    "# if the word appears in one of the columns in our dtm 3\n",
    "# then add the word to our initated list 4\n",
    "\n",
    "flavor_defects_detected = [] # 1\n",
    "\n",
    "for word in flavor_defects: # 2\n",
    "    if word in df.columns: # 3\n",
    "        flavor_defects_detected.append(word) # 4\n",
    "\n",
    "flavor_defects_detected"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a subset of our DTM data frame where the columns are just the flavor defect key words that were detected in our DTM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rotten</th>\n",
       "      <th>bitter</th>\n",
       "      <th>grain</th>\n",
       "      <th>grass</th>\n",
       "      <th>sour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2277</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2278</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2279</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2280</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2281</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2282 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      rotten  bitter  grain  grass  sour\n",
       "0          0       0      0      0     0\n",
       "1          0       0      0      0     0\n",
       "2          0       0      0      0     0\n",
       "3          0       0      0      0     0\n",
       "4          0       0      0      0     0\n",
       "...      ...     ...    ...    ...   ...\n",
       "2277       0       0      0      0     0\n",
       "2278       0       0      0      0     0\n",
       "2279       0       0      0      0     0\n",
       "2280       0       0      0      0     0\n",
       "2281       0       0      0      0     0\n",
       "\n",
       "[2282 rows x 5 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flavor_defects = df[flavor_defects_detected]\n",
    "df_flavor_defects"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a count of the total number of flavor defect key words that were detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jb/__4n695j15169dhq5xqgt4l40000gp/T/ipykernel_69468/52231729.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_flavor_defects[\"n_flavor_defects\"] = df_flavor_defects[flavor_defects_detected].sum(axis = 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rotten</th>\n",
       "      <th>bitter</th>\n",
       "      <th>grain</th>\n",
       "      <th>grass</th>\n",
       "      <th>sour</th>\n",
       "      <th>n_flavor_defects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2277</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2278</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2279</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2280</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2281</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2282 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      rotten  bitter  grain  grass  sour  n_flavor_defects\n",
       "0          0       0      0      0     0                 0\n",
       "1          0       0      0      0     0                 0\n",
       "2          0       0      0      0     0                 0\n",
       "3          0       0      0      0     0                 0\n",
       "4          0       0      0      0     0                 0\n",
       "...      ...     ...    ...    ...   ...               ...\n",
       "2277       0       0      0      0     0                 0\n",
       "2278       0       0      0      0     0                 0\n",
       "2279       0       0      0      0     0                 0\n",
       "2280       0       0      0      0     0                 0\n",
       "2281       0       0      0      0     0                 0\n",
       "\n",
       "[2282 rows x 6 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flavor_defects[\"n_flavor_defects\"] = df_flavor_defects[flavor_defects_detected].sum(axis = 1)\n",
    "df_flavor_defects"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, lets add the new variable to our coffee data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee[\"n_flavor_defects\"] = df_flavor_defects[\"n_flavor_defects\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I reckon that the appearance of these key words may be associated with lower ratings of the coffee. Specifically, the more times these flavor defect key words appear in the description, the lower the score will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2263\n",
       "1      17\n",
       "2       1\n",
       "3       1\n",
       "Name: n_flavor_defects, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coffee[\"n_flavor_defects\"].value_counts().sort_index(ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_flavor_defects\n",
      "0    93.041096\n",
      "1    88.000000\n",
      "2    68.000000\n",
      "3    80.000000\n",
      "Name: rating, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(coffee.groupby([\"n_flavor_defects\"])[\"rating\"].mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly the case with 3 flavor defect key words was rated higher than the case with only 2 key words. Let's see what those words were..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2040    Evaluated at proportions of 5 grams of instant coffee powder mixed with 8.5 ounces (250 ml) of hot water. Not attractive. Rotten suggestions (composted orange, lily) and acrid wood dominate, along with a metallic note. On the positive side, hints of prune, salty caramel, cardamom. Bittersweet and acrid in structure; lean but smooth in mouthfeel. Continued bitter and acrid in the finish.\n",
      "Name: desc_1, dtype: object\n",
      "2040    [evalu, proport, , gram, instant, powder, mix, , ounc, , ml, hot, water, attract, rotten, suggest, compost, orang, lili, acrid, wood, domin, along, metal, note, posit, side, hint, prune, salti, caramel, cardamom, bittersweet, acrid, structur, lean, smooth, mouthfeel, continu, bitter, acrid, finish]\n",
      "Name: desc_1_token_stems, dtype: object\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "print(coffee.loc[coffee[\"n_flavor_defects\"] == 2][\"desc_1\"])\n",
    "print(coffee.loc[coffee[\"n_flavor_defects\"] == 2][\"desc_1_token_stems\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "957    Evaluated at a steeping time of 6 minutes. This beverage is composed of coffee that has essentially been toasted but not roasted. Toasted grain, cocoa nib, hints of raw cashew and limelike citrus in aroma and cup. Sweet, woody/grainy structure with a hint of bitterness but no acidy sensation whatsoever. The mouthfeel is thin and tea-like but silky in texture. Grain and nut fade in the finish, though a woody sweetness lasts.\n",
      "Name: desc_1, dtype: object\n",
      "957    [evalu, steep, time, , minut, beverag, compos, essenti, toast, roast, toast, grain, cocoa, nib, hint, raw, cashew, limelik, citrus, aroma, cup, sweet, woodygraini, structur, hint, bitter, acidi, sensat, whatsoev, mouthfeel, thin, tea-lik, silki, textur, grain, nut, fade, finish, though, woodi, sweet, last]\n",
      "Name: desc_1_token_stems, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(coffee.loc[coffee[\"n_flavor_defects\"] == 3][\"desc_1\"])\n",
    "print(coffee.loc[coffee[\"n_flavor_defects\"] == 3][\"desc_1_token_stems\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inclusion of modifiers (like not) in our list of stop-words impacted this result. The case with 3 flavor defects had one occurance that was modified - \"a hint of bitterness.\" Further, some key words (like rotten) might hold more weight than others. Perhaps a sentiment analysis can better capture if the rating was good or bad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       {'neg': 0.028, 'neu': 0.725, 'pos': 0.247, 'compound': 0.8832}\n",
       "1         {'neg': 0.0, 'neu': 0.799, 'pos': 0.201, 'compound': 0.8316}\n",
       "2         {'neg': 0.0, 'neu': 0.923, 'pos': 0.077, 'compound': 0.4767}\n",
       "3         {'neg': 0.0, 'neu': 0.871, 'pos': 0.129, 'compound': 0.5106}\n",
       "4         {'neg': 0.0, 'neu': 0.761, 'pos': 0.239, 'compound': 0.8176}\n",
       "                                     ...                              \n",
       "2277      {'neg': 0.0, 'neu': 0.842, 'pos': 0.158, 'compound': 0.7346}\n",
       "2278    {'neg': 0.032, 'neu': 0.891, 'pos': 0.077, 'compound': 0.4215}\n",
       "2279      {'neg': 0.0, 'neu': 0.774, 'pos': 0.226, 'compound': 0.9313}\n",
       "2280       {'neg': 0.0, 'neu': 0.838, 'pos': 0.162, 'compound': 0.836}\n",
       "2281    {'neg': 0.029, 'neu': 0.692, 'pos': 0.279, 'compound': 0.9042}\n",
       "Name: polarity_scores, Length: 2282, dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# apply the sentiment analyzer to all the columns\n",
    "coffee[\"polarity_scores\"] = coffee[\"desc_1\"].apply(sentiment_analyzer.polarity_scores)\n",
    "coffee[\"polarity_scores\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result gives us a dictionary in each row. We can therefore supply a key and it will return a value from the dictionary. For example, if we want the negative score, we would use the \"neg\" key and it would return the negative value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.028"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coffee[\"polarity_scores\"][0][\"neg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity_scores</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'neg': 0.028, 'neu': 0.725, 'pos': 0.247, 'compound': 0.8832}</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.8832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'neg': 0.0, 'neu': 0.799, 'pos': 0.201, 'compound': 0.8316}</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.8316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'neg': 0.0, 'neu': 0.923, 'pos': 0.077, 'compound': 0.4767}</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.4767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'neg': 0.0, 'neu': 0.871, 'pos': 0.129, 'compound': 0.5106}</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.5106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'neg': 0.0, 'neu': 0.761, 'pos': 0.239, 'compound': 0.8176}</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.8176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2277</th>\n",
       "      <td>{'neg': 0.0, 'neu': 0.842, 'pos': 0.158, 'compound': 0.7346}</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.7346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2278</th>\n",
       "      <td>{'neg': 0.032, 'neu': 0.891, 'pos': 0.077, 'compound': 0.4215}</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.4215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2279</th>\n",
       "      <td>{'neg': 0.0, 'neu': 0.774, 'pos': 0.226, 'compound': 0.9313}</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.9313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2280</th>\n",
       "      <td>{'neg': 0.0, 'neu': 0.838, 'pos': 0.162, 'compound': 0.836}</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.8360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2281</th>\n",
       "      <td>{'neg': 0.029, 'neu': 0.692, 'pos': 0.279, 'compound': 0.9042}</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.9042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2282 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     polarity_scores  \\\n",
       "0     {'neg': 0.028, 'neu': 0.725, 'pos': 0.247, 'compound': 0.8832}   \n",
       "1       {'neg': 0.0, 'neu': 0.799, 'pos': 0.201, 'compound': 0.8316}   \n",
       "2       {'neg': 0.0, 'neu': 0.923, 'pos': 0.077, 'compound': 0.4767}   \n",
       "3       {'neg': 0.0, 'neu': 0.871, 'pos': 0.129, 'compound': 0.5106}   \n",
       "4       {'neg': 0.0, 'neu': 0.761, 'pos': 0.239, 'compound': 0.8176}   \n",
       "...                                                              ...   \n",
       "2277    {'neg': 0.0, 'neu': 0.842, 'pos': 0.158, 'compound': 0.7346}   \n",
       "2278  {'neg': 0.032, 'neu': 0.891, 'pos': 0.077, 'compound': 0.4215}   \n",
       "2279    {'neg': 0.0, 'neu': 0.774, 'pos': 0.226, 'compound': 0.9313}   \n",
       "2280     {'neg': 0.0, 'neu': 0.838, 'pos': 0.162, 'compound': 0.836}   \n",
       "2281  {'neg': 0.029, 'neu': 0.692, 'pos': 0.279, 'compound': 0.9042}   \n",
       "\n",
       "      positive  negative  compound  \n",
       "0        0.247     0.028    0.8832  \n",
       "1        0.201     0.000    0.8316  \n",
       "2        0.077     0.000    0.4767  \n",
       "3        0.129     0.000    0.5106  \n",
       "4        0.239     0.000    0.8176  \n",
       "...        ...       ...       ...  \n",
       "2277     0.158     0.000    0.7346  \n",
       "2278     0.077     0.032    0.4215  \n",
       "2279     0.226     0.000    0.9313  \n",
       "2280     0.162     0.000    0.8360  \n",
       "2281     0.279     0.029    0.9042  \n",
       "\n",
       "[2282 rows x 4 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initate new lists 1 - 2\n",
    "# for each row in the column polarity scores 3\n",
    "# save that row as a new variable called dictionary 4\n",
    "# append the dictionary value for pos to the pos list 5\n",
    "# append the dictionary value for neg to the pos list 5\n",
    "\n",
    "pos = [] # 1\n",
    "neg = [] # 2\n",
    "compound = [] # 2\n",
    "\n",
    "for each_row in coffee[\"polarity_scores\"]: # 3\n",
    "    dictionary = each_row # 4\n",
    "    pos.append(dictionary[\"pos\"]) # 5\n",
    "    neg.append(dictionary[\"neg\"]) # 6\n",
    "    compound.append(dictionary[\"compound\"]) # 6\n",
    "\n",
    "# use these lists to create new columns in our data frame\n",
    "coffee[\"positive\"] = pos\n",
    "coffee[\"negative\"] = neg\n",
    "coffee[\"compound\"] = compound\n",
    "\n",
    "coffee[[\"polarity_scores\", \"positive\", \"negative\", \"compound\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the average positive sentiment score for each rating? What about negative sentiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating\n",
      "98    0.261000\n",
      "97    0.215571\n",
      "96    0.213833\n",
      "95    0.207279\n",
      "94    0.197660\n",
      "93    0.195565\n",
      "92    0.190475\n",
      "91    0.200472\n",
      "90    0.190642\n",
      "89    0.167200\n",
      "88    0.166000\n",
      "87    0.150000\n",
      "86    0.130100\n",
      "85    0.155500\n",
      "84    0.095250\n",
      "83    0.082333\n",
      "80    0.106000\n",
      "72    0.081000\n",
      "68    0.063000\n",
      "67    0.077000\n",
      "63    0.000000\n",
      "Name: positive, dtype: float64\n",
      "correlation:  0.13182100980038508\n"
     ]
    }
   ],
   "source": [
    "print(coffee.groupby([\"rating\"])[\"positive\"].mean().sort_index(ascending = False))\n",
    "print(\"correlation: \", coffee[\"rating\"].corr(coffee[\"positive\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating\n",
      "63    0.000000\n",
      "67    0.045000\n",
      "68    0.118000\n",
      "72    0.039000\n",
      "80    0.081000\n",
      "83    0.017667\n",
      "84    0.009000\n",
      "85    0.042000\n",
      "86    0.017900\n",
      "87    0.035444\n",
      "88    0.028714\n",
      "89    0.018500\n",
      "90    0.008908\n",
      "91    0.004433\n",
      "92    0.005614\n",
      "93    0.003620\n",
      "94    0.002708\n",
      "95    0.002554\n",
      "96    0.003458\n",
      "97    0.009357\n",
      "98    0.000000\n",
      "Name: negative, dtype: float64\n",
      "correlation:  -0.24143965565822414\n"
     ]
    }
   ],
   "source": [
    "print(coffee.groupby([\"rating\"])[\"negative\"].mean().sort_index(ascending = True))\n",
    "print(\"correlation: \", coffee[\"rating\"].corr(coffee[\"negative\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating\n",
      "63    0.000000\n",
      "67    0.250000\n",
      "68   -0.392300\n",
      "72    0.458800\n",
      "80    0.449700\n",
      "83    0.507433\n",
      "84    0.450700\n",
      "85    0.644600\n",
      "86    0.541480\n",
      "87    0.490067\n",
      "88    0.658543\n",
      "89    0.665360\n",
      "90    0.713469\n",
      "91    0.745744\n",
      "92    0.714472\n",
      "93    0.745590\n",
      "94    0.759546\n",
      "95    0.763565\n",
      "96    0.799199\n",
      "97    0.840950\n",
      "98    0.907433\n",
      "Name: compound, dtype: float64\n",
      "correlation:  0.20268729975090735\n"
     ]
    }
   ],
   "source": [
    "print(coffee.groupby([\"rating\"])[\"compound\"].mean().sort_index(ascending = True))\n",
    "print(\"correlation: \", coffee[\"rating\"].corr(coffee[\"compound\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that negative sentiment is the polarity score that has the strongest association with the coffee ratings, but let's test it and compare it to the key word frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 rating   R-squared:                       0.128\n",
      "Model:                            OLS   Adj. R-squared:                  0.127\n",
      "Method:                 Least Squares   F-statistic:                     166.8\n",
      "Date:                Wed, 05 Apr 2023   Prob (F-statistic):           2.49e-68\n",
      "Time:                        15:24:36   Log-Likelihood:                -4693.0\n",
      "No. Observations:                2282   AIC:                             9392.\n",
      "Df Residuals:                    2279   BIC:                             9409.\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "====================================================================================\n",
      "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------\n",
      "const               93.1329      0.042   2232.627      0.000      93.051      93.215\n",
      "negative           -21.6885      3.039     -7.138      0.000     -27.647     -15.730\n",
      "n_flavor_defects    -4.9547      0.368    -13.465      0.000      -5.676      -4.233\n",
      "==============================================================================\n",
      "Omnibus:                     1848.185   Durbin-Watson:                   1.346\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           142628.591\n",
      "Skew:                          -3.273   Prob(JB):                         0.00\n",
      "Kurtosis:                      41.173   Cond. No.                         76.7\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# regression\n",
    "x = coffee[[\"negative\", \"n_flavor_defects\"]]\n",
    "y = coffee[\"rating\"]\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "model = sm.OLS(y, x, missing = \"drop\")\n",
    "res = model.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average rating for a coffee with a negative sentiment score of 0 and no flavor defect key words detected is 93.13. \n",
    "\n",
    "Both negative sentiment scores and the number of flavor defect key words detected were significant predictors of coffee rating (p < .001). A 1-unit increase in negative sentiment is associated with a 21.68 drop in coffee rating. While each additional key word detected is associated with a 4.95 drop in coffee rating. COOL!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
